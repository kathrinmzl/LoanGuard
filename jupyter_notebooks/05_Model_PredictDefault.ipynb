{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Modelling Loan Defaults**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Fit and evaluate a classification model to predict if a borrower will default or not\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/LoanDefaultData.csv\n",
        "* Instructions on which variables to use for data cleaning and feature engineering. They are found in each respective notebook.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Train set (features and target)\n",
        "* Test set (features and target)\n",
        "* Data cleaning and Feature Engineering pipeline\n",
        "* Modeling pipeline\n",
        "* Feature importance plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">Note: The approach in this notebook is based on the Churnometer Project from Code Institute"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# System & OS\n",
        "import os\n",
        "import warnings  # for ignoring warnings\n",
        "\n",
        "# Ignore FutureWarnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Data Handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "# Inline plotting for VS Code / Jupyter\n",
        "%matplotlib inline  \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")  # consistent style\n",
        "\n",
        "# Machine Learning Utilities\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import (train_test_split, \n",
        "                                     GridSearchCV, \n",
        "                                     RandomizedSearchCV)\n",
        "from sklearn.preprocessing import StandardScaler  # feature scaling\n",
        "from sklearn.feature_selection import SelectFromModel  # feature selection\n",
        "from imblearn.over_sampling import SMOTE  # handle imbalanced datasets\n",
        "\n",
        "# Feature Engineering (Feature-Engine)\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "from feature_engine.imputation import MeanMedianImputer\n",
        "from feature_engine.outliers import Winsorizer\n",
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "from feature_engine import transformation as vt  # log/power transformations\n",
        "\n",
        "# Classification Algorithms\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, \n",
        "                              GradientBoostingClassifier, AdaBoostClassifier)\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Performance Metrics\n",
        "from sklearn.metrics import (classification_report, confusion_matrix, \n",
        "                             recall_score, f1_score, make_scorer)\n",
        "\n",
        "# Joblib for saving/loading models\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder, where the notebook is stored, to its parent folder\n",
        "* First we access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "* Then we want to make the parent of the current directory the new current directory\n",
        "    * os.path.dirname() gets the parent directory\n",
        "    * os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "current_dir = os.getcwd()\n",
        "print(f\"You set a new current directory: {current_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 1: Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the data from the “collection” folder, since we want to design the pipeline to be able to handle the cleaning and engineering by itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"outputs/datasets/collection/LoanDefaultData.csv\")\n",
        "\n",
        "print(df.shape)\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Drop duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.drop_duplicates()\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_var = \"loan_status\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 2: ML Pipeline with all features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. First create ML Pipelines:\n",
        "    * the first three for cleaning and feature engineering \n",
        "    * the fourth for modelling using all available data to predict a borrowers's default. We won’t use this pipeline until a later stage in our notebook, but it’s good practice to create all your pipelines at the start of your notebook\n",
        "\n",
        "2. Next, we split the data into train and test sets\n",
        "\n",
        "3. After that, we can handle the target imbalance, by applying the cleaning and feature engineering pipelines to the data and then resampling the train set target\n",
        "\n",
        "4. Once the data is ready, we fit the modelling pipeline we created using multiple algorithms while using their default hyperparameters, so we can find the algorithms that show the best fit for our data\n",
        "\n",
        "5. Next, for these best given algorithm, we do an extensive optimization search so we can find the best hyperparameters for this algorithm and can identify the best model\n",
        "\n",
        "6. After that we’ll need to assess the best model's most relevant features from the second pipeline\n",
        "\n",
        "7. And finally, we evaluate the pipeline using the best model on the train and test sets to check if it meets the requirements we set in our business case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ML pipeline for Data Cleaning and Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we apply what we know from the previous feature engineering step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineDataClAndFeatureEngPart1():\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"MedianImputer\", MeanMedianImputer(\n",
        "            imputation_method='median',\n",
        "            variables=['person_emp_length', 'loan_int_rate']\n",
        "        )),\n",
        "        (\"Winsorizer\", Winsorizer(\n",
        "            capping_method='iqr',\n",
        "            fold=5,\n",
        "            tail='right',\n",
        "            variables=[\n",
        "                'person_age', 'person_income', 'person_emp_length',\n",
        "                'loan_amnt', 'loan_int_rate', 'loan_percent_income',\n",
        "                'cb_person_cred_hist_length'\n",
        "            ]\n",
        "        )),\n",
        "        ('log_transform', vt.LogTransformer(\n",
        "            variables=[\n",
        "                'person_age', 'person_income', 'cb_person_cred_hist_length'\n",
        "            ]\n",
        "        )),\n",
        "        ('power_transform', vt.PowerTransformer(\n",
        "            variables=[\n",
        "                'person_emp_length', 'loan_amnt', 'loan_percent_income'\n",
        "            ]\n",
        "        )),\n",
        "        (\"OrdinalEncoderArbitrary\", OrdinalEncoder(\n",
        "            encoding_method='arbitrary',\n",
        "            variables=['person_home_ownership', 'loan_intent', \n",
        "                    'cb_person_default_on_file'],\n",
        "            unseen='encode'\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    return pipeline_base\n",
        "\n",
        "PipelineDataClAndFeatureEngPart1()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The categorical encoder for `loan_grade` has to be directly applied on the `loan_grade` column and therefore has to be applied seperately from the other feature engineering steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineDataClAndFeatureEngPart2():\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"OrdinalEncoderOrdered\", OrdinalEncoder(\n",
        "            encoding_method='ordered', \n",
        "            variables=\"loan_grade\", \n",
        "            unseen='encode'))\n",
        "    ])\n",
        "\n",
        "    return pipeline_base\n",
        "\n",
        "PipelineDataClAndFeatureEngPart2()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After all the previous transformations we still need to apply SmartCorrelatedSelection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def PipelineDataClAndFeatureEngPart3():\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"SmartCorrelatedSelection\", \n",
        "         SmartCorrelatedSelection(variables=None, \n",
        "                                  method=\"spearman\", \n",
        "                                  threshold=0.6, \n",
        "                                  selection_method=\"variance\"))\n",
        "    ])\n",
        "\n",
        "    return pipeline_base\n",
        "\n",
        "PipelineDataClAndFeatureEngPart3()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ML Pipeline for Modelling and Hyperparameter Optimisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineClf(model):\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"feat_selection\", SelectFromModel(model)),\n",
        "        (\"model\", model),\n",
        "    ])\n",
        "\n",
        "    return pipeline_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split Train and Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At this point, we use all 11 features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df.drop([target_var], axis=1),\n",
        "    df[target_var],\n",
        "    test_size=0.2,\n",
        "    random_state=0,\n",
        ")\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Handle Target Imbalance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From our previous analysis we know we have to handle target imbalance to improve performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preparation: Data Cleaning and Feature Engineering\n",
        "\n",
        "First apply PipelineDataClAndFeatureEngPart1(), PipelineDataClAndFeatureEngPart2() and PipelineDataClAndFeatureEngPart3(), because for resampling, the data can’t contain missing values\n",
        "or categorical values. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_data_cl_feat_eng_part1 = PipelineDataClAndFeatureEngPart1()\n",
        "X_train = pipeline_data_cl_feat_eng_part1.fit_transform(X_train)\n",
        "X_test = pipeline_data_cl_feat_eng_part1.transform(X_test)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_data_cl_feat_eng_part2 = PipelineDataClAndFeatureEngPart2()\n",
        "X_train[\"loan_grade\"] = (\n",
        "    pipeline_data_cl_feat_eng_part2\n",
        "    .fit_transform(X_train[[\"loan_grade\"]],y_train)\n",
        "    )\n",
        "X_test[\"loan_grade\"] = (\n",
        "    pipeline_data_cl_feat_eng_part2\n",
        "    .transform(X_test[[\"loan_grade\"]])\n",
        "    )\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_data_cl_feat_eng_part3 = PipelineDataClAndFeatureEngPart3()\n",
        "X_train = pipeline_data_cl_feat_eng_part3.fit_transform(X_train)\n",
        "X_test = pipeline_data_cl_feat_eng_part3.transform(X_test)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After applying the data cleaning and feature engineering pipeline, the number of features decreased from 11 to 8, because of ``SmartCorrelatedSelection``."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Apply SMOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check current Train Set Target distribution to confirm the target is currently imbalanced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.set_style(\"whitegrid\")\n",
        "y_train.value_counts().plot(kind='bar', title='Train Set Target Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use SMOTE (Synthetic Minority Oversampling Technique) to balance the Train Set target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "oversample = SMOTE(sampling_strategy='minority', random_state=0)\n",
        "X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check Train Set Target distribution after resampling and confirm that the target is now balanced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train.value_counts().plot(kind='bar', title='Train Set Target Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grid Search CV - Sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This step is divided into 2 subsections:\n",
        "* First, we will fit a set of pipelines for each algorithm using the default hyperparameters,\n",
        "to find the algorithm that most suits the data\n",
        "* Then, we will do an extensive hyperparameter search using this algorithm\n",
        "\n",
        "This combined approach is faster than doing one extensive search on all algorithms at once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Custom Class for Hyperparameter Optimisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the custom class introduced by Code Institute in the \"Churnometer\" walkthrough project to assist with hyperparameter optimization. We customized the class to support both RandomizedSearchCV and GridSearchCV. The RandomizedSearchCV option was added to reduce computation time during the extensive hyperparameter search.\n",
        "\n",
        "The class automates the hyperparameter tuning process across multiple models, each with its own parameter grid.\n",
        "\n",
        "In the fit() method, each model is wrapped in a custom PipelineClf() and passed to GridSearchCV/RandomizedSearchCV, which performs an exhaustive search over the defined hyperparameter combinations using cross-validation. The fitted grid search objects are stored for later analysis.\n",
        "\n",
        "The score_summary() method then compiles the cross-validation results into a unified DataFrame, summarizing each model’s minimum, maximum, mean, and standard deviation of scores, enabling quick and consistent comparison across all tested algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Customized from Churnometer Walkthrough Project\n",
        "class HyperparameterOptimizationSearch:\n",
        "\n",
        "    def __init__(self, models, params):\n",
        "        self.models = models\n",
        "        self.params = params\n",
        "        self.keys = models.keys()\n",
        "        self.grid_searches = {}\n",
        "\n",
        "    def fit(self, X, y, cv, n_jobs, n_iter=30, randomized=False, \n",
        "            verbose=1, scoring=None, refit=False):\n",
        "        \n",
        "        for key in self.keys:\n",
        "\n",
        "            model = PipelineClf(self.models[key])\n",
        "            params = self.params[key]\n",
        "            if randomized == True:\n",
        "                print(f\"\\nRunning RandomizedSearchCV for {key} \\n\")\n",
        "                gs = RandomizedSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
        "                                verbose=verbose, scoring=scoring, n_iter=n_iter)\n",
        "            else: \n",
        "                print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
        "                gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs, \n",
        "                                  verbose=verbose, scoring=scoring, )\n",
        "            gs.fit(X, y)\n",
        "            self.grid_searches[key] = gs\n",
        "\n",
        "    def score_summary(self, sort_by='mean_score'):\n",
        "        def row(key, scores, params):\n",
        "            d = {\n",
        "                'estimator': key,\n",
        "                'min_score': min(scores),\n",
        "                'max_score': max(scores),\n",
        "                'mean_score': np.mean(scores),\n",
        "                'std_score': np.std(scores),\n",
        "            }\n",
        "            return pd.Series({**params, **d})\n",
        "\n",
        "        rows = []\n",
        "        for k in self.grid_searches:\n",
        "            params = self.grid_searches[k].cv_results_['params']\n",
        "            scores = []\n",
        "            for i in range(self.grid_searches[k].cv):\n",
        "                key = \"split{}_test_score\".format(i)\n",
        "                r = self.grid_searches[k].cv_results_[key]\n",
        "                scores.append(r.reshape(len(params), 1))\n",
        "\n",
        "            all_scores = np.hstack(scores)\n",
        "            for p, s in zip(params, all_scores):\n",
        "                rows.append((row(k, s, p)))\n",
        "\n",
        "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
        "        columns = ['estimator', 'min_score',\n",
        "                   'mean_score', 'max_score', 'std_score']\n",
        "        columns = columns + [c for c in df.columns if c not in columns]\n",
        "        return df[columns], self.grid_searches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Custom Performance Metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To evaluate model performance we use a hybrid score combining recall and F1 because the target is imbalanced and catching defaults is critical. This ensures the model prioritizes identifying true defaults (high recall) while still considering overall predictive quality (F1), guiding hyperparameter tuning toward a practical, business-relevant performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hybrid_score(y_true, y_pred):\n",
        "    recall = recall_score(y_true, y_pred, pos_label=1)\n",
        "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "    return 0.7 * recall + 0.3 * f1_macro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use standard hyperparameters to find most suitable algorithms\n",
        "\n",
        "#### Define algorithms for Quick Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models_quick_search = {\n",
        "    # Linear models\n",
        "    \"LogisticRegression\": LogisticRegression(random_state=0),\n",
        "    \"RidgeClassifier\": RidgeClassifier(random_state=0),\n",
        "\n",
        "    #Tree-based models\n",
        "    \"DecisionTreeClassifier\": DecisionTreeClassifier(random_state=0),\n",
        "    \"RandomForestClassifier\": RandomForestClassifier(random_state=0),\n",
        "    \"ExtraTreesClassifier\": ExtraTreesClassifier(random_state=0),\n",
        "\n",
        "    # Boosting methods \n",
        "    \"XGBClassifier\": XGBClassifier(random_state=0),\n",
        "    \"CatBoostClassifier\": CatBoostClassifier(random_state=0, verbose=0),\n",
        "    \"AdaBoostClassifier\": AdaBoostClassifier(random_state=0),\n",
        "    \"GradientBoostingClassifier\": GradientBoostingClassifier(random_state=0)\n",
        "}\n",
        "\n",
        "# Set empty dictionaries to indicated that we use the default hyperparameters\n",
        "params_quick_search = {\n",
        "    \"LogisticRegression\": {},\n",
        "    \"RidgeClassifier\": {},\n",
        "    \"XGBClassifier\": {},\n",
        "    \"CatBoostClassifier\": {},\n",
        "    \"DecisionTreeClassifier\": {},\n",
        "    \"RandomForestClassifier\": {},\n",
        "    \"GradientBoostingClassifier\": {},\n",
        "    \"ExtraTreesClassifier\": {},\n",
        "    \"AdaBoostClassifier\": {},\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For our binary classification task, we select a diverse set of models that cover different algorithmic families and learning principles. This allows us to compare performance across linear, tree-based, and ensemble approaches:\n",
        "\n",
        "* Linear Models:  \n",
        "Serve as strong and interpretable baselines. They model linear relationships between features and the target and provide useful insights into feature importance.\n",
        "\n",
        "* Tree-Based Models:  \n",
        "Capture nonlinear relationships and feature interactions without requiring scaling or transformation. Random Forest and Extra Trees improve generalization by aggregating multiple decision trees.\n",
        "\n",
        "* Boosting Methods:  \n",
        "Build powerful ensembles by sequentially improving weak learners. These algorithms often achieve high accuracy on structured data and handle complex feature relationships effectively.\n",
        "\n",
        "This model mix ensures a comprehensive benchmark, from simple and interpretable models to complex and high-performing ensemble methods, helping us identify the most suitable approach for our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Quick GridSearch CV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we perform a quick GridSearchCV to evaluate the model performance of the different algorithms using the default hyperparameters.\n",
        "\n",
        "We use 5-fold cross-validation to obtain a robust estimate of model performance and reduce the impact of random variation in the training data. It is a rule of thumb used in data science as a starting point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search = HyperparameterOptimizationSearch(models=models_quick_search, \n",
        "                                          params=params_quick_search)\n",
        "search.fit(X_train, y_train,\n",
        "           scoring = make_scorer(hybrid_score),\n",
        "           n_jobs=-1, cv=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Check results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_summary, grid_search_pipelines = (search\n",
        "                                              .score_summary(\n",
        "                                                  sort_by='mean_score'))\n",
        "grid_search_summary "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Among the tested algorithms, the best-performing model is the ``RandomForestClassifier``, achieving a mean Recall score of 0.847. The second-best model is the ``DecisionTreeClassifier``, with a Recall score of 0.832.\n",
        "\n",
        "In a professional setting, we could continue tuning hyperparameters for both models, as their scores are both high and offer strong potential. However, to reduce computing time while still meeting the project requirements defined by Code Institute, we will focus only on the ``RandomForestClassifier`` for the next steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Do an extensive search on the RandomForestClassifier to find the best hyperparameter configuration\n",
        "\n",
        "Now we will perform an extensive grid search using the RandomForestClassifier to optimize its hyperparameters for the best possible performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define algorithm and hyperparameters for Extensive Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models_search = {\n",
        "    \"RandomForestClassifier\":RandomForestClassifier(random_state=0)\n",
        "}\n",
        "\n",
        "# Documentation to help on hyperparameter list: \n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "# https://www.geeksforgeeks.org/machine-learning/hyperparameters-of-random-forest-classifier/\n",
        "   \n",
        "params_search = {\n",
        "    \"RandomForestClassifier\": {\n",
        "        'model__n_estimators': [500, 800, 1000],  \n",
        "        'model__criterion': [\"gini\", \"log_loss\", \"entropy\"],      \n",
        "        'model__max_depth': [6, 8, 10],           \n",
        "        'model__min_samples_split': [2, 5, 10],         \n",
        "        'model__min_samples_leaf': [1, 3, 5],           \n",
        "        'model__max_features': [\"sqrt\", \"log2\", 0.5],   \n",
        "        'model__class_weight': ['balanced', None]       \n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Extensive GridSearch CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search = HyperparameterOptimizationSearch(models=models_search, \n",
        "                                          params=params_search)\n",
        "\n",
        "# use RandomizedSearchCV to reduce computing time\n",
        "search.fit(X_train, y_train,\n",
        "           scoring = make_scorer(hybrid_score),\n",
        "           n_jobs=-1, cv=5, randomized=True, n_iter=50) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Check results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_summary, grid_search_pipelines = (search\n",
        "                                              .score_summary(\n",
        "                                                  sort_by='mean_score'))\n",
        "grid_search_summary.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get best model name programmatically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = grid_search_summary.iloc[0,0]\n",
        "best_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The best RandomForestClassifier model has a mean score of 0.8 and the following hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_parameters = grid_search_pipelines[best_model].best_params_\n",
        "best_parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract the best classification pipeline from the GridSearchCV object, which includes the preprocessing steps and the model fitted with the optimal hyperparameters found during the grid search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_clf = grid_search_pipelines[best_model].best_estimator_\n",
        "pipeline_clf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assess feature importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our classification pipeline consists of three steps:\n",
        "\n",
        "1. Feature scaling – standardizes the numerical variables.\n",
        "2. Feature selection – selects the most relevant features for the model.\n",
        "3. Modelling – fits the RandomForestClassifier on the selected features.\n",
        "\n",
        "We can extract two pieces of information from the pipeline:\n",
        "\n",
        "* Selected features: which features passed through the feature selection step and are actually used by the model.\n",
        "* Feature importance: the importance scores of these features as calculated by the Random Forest in the modelling step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_features = (X_train.columns[pipeline_clf['feat_selection']\n",
        "                                     .get_support()])\n",
        "print(f\"Number of selected features: {len(selected_features)}\")\n",
        "selected_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Feature importance:\")\n",
        "pipeline_clf['model'].feature_importances_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create a DataFrame that contains these features' importance and plot it as a bar plot, to show the most important features for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_feature_importance = (pd.DataFrame(data={\n",
        "    'Feature': X_train.columns[pipeline_clf['feat_selection'].get_support()],\n",
        "    'Importance': pipeline_clf['model'].feature_importances_})\n",
        "    .sort_values(by='Importance', ascending=False)\n",
        ")\n",
        "\n",
        "# re-assign best_features order\n",
        "best_features = df_feature_importance['Feature'].to_list()\n",
        "\n",
        "# Most important features statement and plot\n",
        "print(\n",
        "    f\"* These are the {len(best_features)} most important features in \"\n",
        "    f\"descending order. The model was trained on them:\\n\"\n",
        "    f\"{df_feature_importance['Feature'].to_list()}\"\n",
        ")\n",
        "\n",
        "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the feature importance analysis, only three features show substantial predictive power for loan default:\n",
        "\n",
        "* ``person_income``: The most influential feature. Borrowers’ income strongly affects the model’s prediction of default risk, with higher income generally reducing default probability.\n",
        "* ``loan_int_rate``: Interest rate is another key predictor. Higher interest rates are associated with increased default likelihood.\n",
        "This also reflects the risk associated with a borrower, because interest rates are tied to loan grade; borrowers with lower/worse loan grades receive higher rates, signaling higher risk.\n",
        "* ``loan_amnt``: Loan amount also plays a significant role. Larger loans increase default risk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Pipeline on Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# From Churnometer Walkthrough Project\n",
        "def confusion_matrix_and_report(X, y, pipeline, label_map):\n",
        "\n",
        "    prediction = pipeline.predict(X)\n",
        "\n",
        "    print('---  Confusion Matrix  ---')\n",
        "    print(pd.DataFrame(confusion_matrix(y_true=y, y_pred=prediction),\n",
        "          columns=[[\"Prediction \" + sub for sub in label_map]],\n",
        "          index=[[\"Actual \" + sub for sub in label_map]]\n",
        "          ))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print('---  Classification Report  ---')\n",
        "    print(classification_report(y, prediction, target_names=label_map), \"\\n\")\n",
        "\n",
        "\n",
        "def clf_performance(X_train, y_train, X_test, y_test, pipeline, label_map):\n",
        "    print(\"#### Train Set #### \\n\")\n",
        "    confusion_matrix_and_report(X_train, y_train, pipeline, label_map)\n",
        "\n",
        "    print(\"#### Test Set ####\\n\")\n",
        "    confusion_matrix_and_report(X_test, y_test, pipeline, label_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluation: We cross check with the metrics defined in the ML business case\n",
        "- Recall for default ≥ 0.75 – to minimize false negatives (high-risk borrowers predicted as safe)\n",
        "- F1 score ≥ 0.60 – ensures a balance between recall and precision\n",
        "\n",
        "Metrics should be met both on training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf_performance(X_train=X_train, y_train=y_train,\n",
        "                X_test=X_test, y_test=y_test,\n",
        "                pipeline=pipeline_clf,\n",
        "                label_map= ['No Default', 'Default'] \n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model achieves a recall of 0.82 on the training set and 0.80 on the test set, both above the business threshold of 0.75, ensuring that high-risk borrowers are correctly identified. \n",
        "\n",
        "The F1 score is 0.80 on training and 0.61 on testing, showing a drop due to lower precision on unseen data but still above the minimum requirement of 0.60.\n",
        "\n",
        "Performance is reasonably consistent between training and test sets, and the high recall confirms the model’s effectiveness at capturing defaulters, fulfilling the key business objectives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 3: Refit pipeline with best features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To prepare the model for deployment as a real-world system, we refit the pipeline using only the three most important features identified during feature importance analysis. This step ensures that the deployed model remains accurate, interpretable, and efficient, focusing only on the variables that truly drive default predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Refit ML Pipeline and Resampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In theory, a pipeline fitted using only the most important features should produce similar results to one fitted with all variables and feature selection.\n",
        "\n",
        "However, in this project we apply SMOTE to balance the training set. This resampling slightly alters the data, so some differences in performance are expected. Nevertheless, we do not expect large deviations, and overall model behavior should remain consistent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rewrite ML pipeline for Data Cleaning and Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create a new pipeline using only the three most important features.\n",
        "\n",
        "Since we now focus on these key features, SmartCorrelatedSelection is no longer needed, simplifying the pipeline and making it more suitable for deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineDataClAndFeatureEng():\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"MedianImputer\", MeanMedianImputer(imputation_method='median', \n",
        "                                            variables=['loan_int_rate'])),\n",
        "        (\"Winsorizer\", Winsorizer(capping_method='iqr', fold=5, tail='right', \n",
        "                                  variables=['person_income', 'loan_amnt', \n",
        "                                             'loan_int_rate'])),\n",
        "        ('log_transform', vt.LogTransformer(variables=['person_income',])),\n",
        "        ('power_transform', vt.PowerTransformer(variables=['loan_amnt']))\n",
        "    ])\n",
        "\n",
        "    return pipeline_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rewrite ML Pipeline for Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since feature selection is no longer required, SelectFromModel is removed from the modelling pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineClf(model):\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"model\", model),\n",
        "    ])\n",
        "\n",
        "    return pipeline_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split Train Test Set, considering only with best features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df.drop([target_var], axis=1),\n",
        "    df[target_var],\n",
        "    test_size=0.2,\n",
        "    random_state=0,\n",
        ")\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We filter only the most important variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = X_train.filter(best_features)\n",
        "X_test = X_test.filter(best_features)\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "X_train.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Handle Target Imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_data_cl_feat_eng = PipelineDataClAndFeatureEng()\n",
        "X_train = pipeline_data_cl_feat_eng.fit_transform(X_train)\n",
        "X_test = pipeline_data_cl_feat_eng.transform(X_test)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check Train Set Target distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "y_train.value_counts().plot(kind='bar', title='Train Set Target Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use SMOTE to balance Train Set target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "oversample = SMOTE(sampling_strategy='minority', random_state=0)\n",
        "X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check Train Set Target distribution after SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train.value_counts().plot(kind='bar',title='Train Set Target Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grid Search CV: Sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we will conduct another grid search using the most suitable model from the last section and its best hyperparameter configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model from last grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models_search "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the best parameters from the last grid search "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save best hyperparameter values as a list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params_search = {'RandomForestClassifier':  {\n",
        "    'model__n_estimators': [1000],\n",
        "    'model__min_samples_split': [5],\n",
        "    'model__min_samples_leaf': [3],\n",
        "    'model__max_features': ['sqrt'],\n",
        "    'model__max_depth': [6],\n",
        "    'model__criterion': ['gini'],\n",
        "    'model__class_weight': [None]\n",
        "    }\n",
        "}\n",
        "params_search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GridSearch CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "quick_search = HyperparameterOptimizationSearch(models=models_search, \n",
        "                                                params=params_search)\n",
        "quick_search.fit(X_train, y_train,\n",
        "           scoring = make_scorer(hybrid_score),\n",
        "           n_jobs=-1, cv=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The summary performance of the reduced-feature pipeline is 0.79, slightly lower than the previous 0.80.\n",
        "\n",
        "This difference is expected because SMOTE resampling was applied using only the three selected features, whereas previously all variables were used. The resulting training set is therefore slightly different, leading to a small change in pipeline performance.\n",
        "\n",
        "Overall, the pipeline remains robust, and we consider this our best ML pipeline for deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_summary, grid_search_pipelines = (quick_search\n",
        "                                              .score_summary(\n",
        "                                                  sort_by='mean_score'))\n",
        "grid_search_summary "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the best clf pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = grid_search_summary.iloc[0, 0]\n",
        "pipeline_clf = grid_search_pipelines[best_model].best_estimator_\n",
        "pipeline_clf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assess feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# We don't have feature selection,\n",
        "# so the best features are the columns from the train set.\n",
        "best_features = X_train.columns\n",
        "\n",
        "# create DataFrame to display feature importance\n",
        "df_feature_importance = (pd.DataFrame(data={\n",
        "    'Feature': best_features,\n",
        "    'Importance': pipeline_clf['model'].feature_importances_})\n",
        "    .sort_values(by='Importance', ascending=False)\n",
        ")\n",
        "\n",
        "# Most important features statement and plot\n",
        "print(\n",
        "    f\"* These are the {len(best_features)} most important features in \"\n",
        "    f\"descending order. The model was trained on them:\\n\"\n",
        "    f\"{df_feature_importance['Feature'].to_list()}\"\n",
        ")\n",
        "\n",
        "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The features have the same relevance as before"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Pipeline on Train and Test Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluation: We cross check with the metrics defined in the ML business case\n",
        "- Recall for default ≥ 0.75 – to minimize false negatives (high-risk borrowers predicted as safe)\n",
        "- F1 score ≥ 0.60 – ensures a balance between recall and precision\n",
        "\n",
        "Metrics should be met both on training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf_performance(X_train=X_train, y_train=y_train,\n",
        "                X_test=X_test, y_test=y_test,\n",
        "                pipeline=pipeline_clf,\n",
        "                label_map= ['No Default', 'Default'] \n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model achieves a recall of 0.80 on the training set and 0.78 on the test set, both above the business threshold of 0.75, ensuring that high-risk borrowers are correctly identified. \n",
        "\n",
        "The F1 score is 0.80 on training and 0.62 on testing, showing a drop due to lower precision on unseen data but still above the minimum requirement of 0.60.\n",
        "\n",
        "Performance is reasonably consistent between training and test sets, and the high recall confirms the model’s effectiveness at capturing defaulters, fulfilling the key business objectives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Overall the results are very similar to the results we previously had for the pipeline with all features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 4: Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will generate the following files\n",
        "* Train set\n",
        "* Test set\n",
        "* Data cleaning and Feature Engineering pipeline\n",
        "* Modeling pipeline\n",
        "* features importance plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, we set a version tag, which will be the folder name. \n",
        "# It's appropriate since it's a form\n",
        "# of version control.\n",
        "version = 'v2'\n",
        "file_path = f'outputs/ml_pipeline/predict_default/{version}'\n",
        "\n",
        "# create a folder to store the files.\n",
        "try:\n",
        "    os.makedirs(name=file_path)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will save the train and test set since we need them to evaluate the model in the dashboard.\n",
        "\n",
        "Note that the data cleaning and feature engineering\n",
        "pipeline have transformed the features. Also SMOTE has already been applied to the train set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train.to_csv(f\"{file_path}/X_train.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train.to_csv(f\"{file_path}/y_train.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(X_test.shape)\n",
        "X_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test.to_csv(f\"{file_path}/X_test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_test.to_csv(f\"{file_path}/y_test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ML Pipelines: Data Cleaning and Feat Eng pipeline and Modelling Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will save 2 pipelines: \n",
        "* Both should be used in conjunction to predict Live Data.\n",
        "* To predict on Train Set, Test Set we use only pipeline_clf, since the data is already processed.\n",
        "\n",
        "\n",
        "\n",
        "Pipeline responsible for Data Cleaning and Feature Engineering.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_data_cl_feat_eng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump(value=pipeline_data_cl_feat_eng ,\n",
        "            filename=f\"{file_path}/clf_pipeline_data_cl_feat_eng.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pipeline responsible for Feature Scaling, and Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_clf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump(value=pipeline_clf ,\n",
        "            filename=f\"{file_path}/clf_pipeline_model.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Importance plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_feature_importance.plot(kind='bar',x='Feature',y='Importance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
        "plt.savefig(f'{file_path}/features_importance.png', bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Conclusions and Next Steps\n",
        "\n",
        "We developed a predictive model to estimate the likelihood of borrower default based on key financial and personal features. \n",
        "The performance of the model is reasonably consistent between training and test sets, and the high recall confirms the model’s effectiveness at capturing defaulters, fulfilling the key business objectives.\n",
        "\n",
        "Next Steps:\n",
        "\n",
        "* As a next step, we will perform a cluster analysis to segment borrowers based on shared characteristics. Combining the prediction model with cluster insights in the dashboard will provide a deeper business understanding, enabling more targeted risk management and tailored strategies for different borrower segments."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
