{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Feature Engineering Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "*  Evaluate which transformations are beneficial for our dataset\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* inputs/datasets/cleaned/TrainSet.csv\n",
        "* inputs/datasets/cleaned/TestSet.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Generate a list of engineering approaches for each variable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "# for vs code\n",
        "%matplotlib inline \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from ydata_profiling import ProfileReport\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "from feature_engine import transformation as vt\n",
        "import scipy.stats as stats\n",
        "# Ignore FutureWarnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder, where the notebook is stored, to its parent folder\n",
        "* First we access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "* Then we want to make the parent of the current directory the new current directory\n",
        "    * os.path.dirname() gets the parent directory\n",
        "    * os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "current_dir = os.getcwd()\n",
        "print(f\"You set a new current directory: {current_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Cleaned Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path = \"outputs/datasets/cleaned\"\n",
        "\n",
        "TrainSet = pd.read_csv(f\"{file_path}/TrainSet.csv\")\n",
        "TrainSet.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TestSet = pd.read_csv(f\"{file_path}/TestSet.csv\")\n",
        "TestSet.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To identify potential transformations, we first revisit the Profile Report generated earlier.\n",
        "This allows us to assess:\n",
        "\n",
        "* The distributions of numerical variables (to detect skewness or outliers)\n",
        "* The cardinality and balance of categorical variables\n",
        "* Possible data scaling needs due to large differences in magnitude\n",
        "\n",
        "Based on these insights, we will determine which variables may benefit from transformations such as scaling, normalization or encoding before modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert object columns to categorical so that it can be displayed \n",
        "# properly in the report\n",
        "TrainSet_cat = TrainSet.copy()\n",
        "for col in TrainSet_cat.select_dtypes(include='object').columns:\n",
        "    TrainSet_cat[col] = TrainSet_cat[col].astype('category')\n",
        "    \n",
        "pandas_report = ProfileReport(df=TrainSet_cat, minimal=True)\n",
        "pandas_report.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Profile Report suggests different transformations depending on the variable type and distribution.\n",
        "\n",
        "Categorical variables should be handled differently based on whether they are nominal or ordinal. The numerical variables are mostly skewed, so numerical transformations may help improve model performance. Additionally, correlated features should be identified for possible removal to reduce redundancy. Finally, variables should be scaled to ensure comparable ranges, which benefits many machine learning algorithms. \n",
        "\n",
        "Transformation Steps:\n",
        "1. Categorical variables\n",
        "    * Nominal Variables: OrdinalEncoder (arbitrary)\n",
        "        * `person_home_ownership`, `loan_intent`, `cb_person_default_on_file`\n",
        "    * Ordinal Variables: OrdinalEncoder (ordered)\n",
        "        * `loan_grade`\n",
        "2. Numerical Variables\n",
        "    * All other numerical variables: Numerical transformation, since they do not have a normal distribution \n",
        "3. All Variables: Smart correlated selection, so any correlated features will be removed\n",
        "4. Scaling variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set the target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_var = \"loan_status\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Categorical Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Nominal Variables: OrdinalEncoder (arbitrary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 1: Select variables and create a separate DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "variables_engineering= (TrainSet\n",
        "                        .select_dtypes(include='object')\n",
        "                        .columns.drop(\"loan_grade\").tolist())\n",
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "df_engineering.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 2: Create engineered variables by applying the encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder = OrdinalEncoder(\n",
        "    encoding_method='arbitrary', \n",
        "    variables=variables_engineering)\n",
        "df_feat_eng = encoder.fit_transform(df_engineering)\n",
        "df_feat_eng.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 3: Assess transformation by comparing engineered variables distribution to original ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for var in variables_engineering:\n",
        "    # Show encoding order\n",
        "    print(f\"Encoding Dict: {var}\")\n",
        "    print(encoder.encoder_dict_[var])\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
        "    fig.suptitle(f\"Distribution Comparison for '{var}' (Raw vs Encoded)\", \n",
        "                 fontsize=14, y=1.05)\n",
        "\n",
        "    # Raw variable countplot\n",
        "    sns.countplot(data=df_engineering, x=var, palette=\"Set2\", ax=axes[0])\n",
        "    axes[0].set_title(\"Raw Variable\")\n",
        "    axes[0].set_xlabel(\"\")\n",
        "    axes[0].tick_params(axis=\"x\", rotation=90)\n",
        "\n",
        "    # Encoded variable bar plot\n",
        "    sns.countplot(data=df_feat_eng, x=var, palette=\"Set2\", ax=axes[1])\n",
        "    axes[1].set_title(\"Encoded Variable\")\n",
        "    axes[1].set_xlabel(\"\")\n",
        "    axes[1].tick_params(axis=\"x\", rotation=90)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* For nominal categorical variables, the OrdinalEncoder successfully transformed each category into a numeric code. Therefore, this transformation will be applied in the pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 4 - Apply the selected transformation to the Train and Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder = OrdinalEncoder(\n",
        "    encoding_method='arbitrary', \n",
        "    variables=variables_engineering)\n",
        "TrainSet = encoder.fit_transform(TrainSet)\n",
        "TestSet = encoder.fit_transform(TestSet)\n",
        "\n",
        "TrainSet.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ordinal Variables: OrdinalEncoder (ordered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 1: Select variables and create a separate DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "variables_engineering= [\"loan_grade\", target_var]\n",
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "df_engineering.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 2: Create engineered variables by applying the encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder = OrdinalEncoder(encoding_method='ordered', variables=\"loan_grade\")\n",
        "df_feat_eng[\"loan_grade\"]  = (\n",
        "    encoder.fit_transform(df_engineering[[\"loan_grade\"]], \n",
        "                          df_engineering[target_var])\n",
        "    )\n",
        "df_feat_eng.head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 3: Assess transformation by comparing engineered variables distribution to original ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "var = variables_engineering[0]\n",
        "\n",
        "# Show encoding order\n",
        "print(f\"Encoding Dict: {var}\")\n",
        "print(encoder.encoder_dict_[var])\n",
        "    \n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
        "fig.suptitle(f\"Distribution Comparison for '{var}' (Raw vs Encoded)\", \n",
        "             fontsize=14, y=1.05)\n",
        "\n",
        "# Raw variable countplot\n",
        "sns.countplot(data=df_engineering, x=var, palette=\"Set2\", ax=axes[0], \n",
        "              order= df_engineering[var].value_counts().index)\n",
        "axes[0].set_title(\"Raw Variable (Countplot)\")\n",
        "axes[0].set_xlabel(\"\")\n",
        "axes[0].tick_params(axis=\"x\", rotation=90)\n",
        "\n",
        "# Encoded variable bar plot\n",
        "sns.countplot(data=df_feat_eng, x=var, palette=\"Set2\", ax=axes[1])\n",
        "axes[1].set_title(\"Encoded Variable (Countplot)\")\n",
        "axes[1].set_xlabel(\"\")\n",
        "axes[1].tick_params(axis=\"x\", rotation=90)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The OrdinalEncoder successfully transformed the ordered categorical variables into numeric form while preserving their order. Therefore, this transformation will be applied in the pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 4 - Apply the selected transformation to the Train and Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder = OrdinalEncoder(encoding_method='ordered', \n",
        "                         variables=\"loan_grade\")\n",
        "TrainSet[\"loan_grade\"] = encoder.fit_transform(TrainSet[[\"loan_grade\"]], \n",
        "                                               TrainSet[target_var])\n",
        "TestSet[\"loan_grade\"] = encoder.transform(pd.DataFrame(TestSet[\"loan_grade\"]))\n",
        "TrainSet.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Numerical Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Custom function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following function gets a DataFrame as input and applies a defined set of numerical\n",
        "feature engineering transformations. This will help to decide which transformations to apply to the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Customized from Churnometer Walkthrough Project\n",
        "def FeatureEngineeringAnalysis(df, analysis_type=None):\n",
        "    \"\"\"\n",
        "    Perform quick feature engineering on numerical variables to evaluate \n",
        "    transformations that improve distribution shapes.\n",
        "\n",
        "    Steps:\n",
        "    - Checks for missing values and valid analysis type.\n",
        "    - Applies multiple numerical transformations (log, power, etc.).\n",
        "    - Visualizes each transformed variable’s distribution and normality.\n",
        "    \n",
        "    Parameters:\n",
        "        df (pd.DataFrame): Input dataset.\n",
        "        analysis_type (str): Currently supports 'numerical' only.\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: Dataset with additional transformed features.\n",
        "    \"\"\"\n",
        "    check_missing_values(df)\n",
        "    allowed_types = ['numerical']\n",
        "    check_user_entry_on_analysis_type(analysis_type, allowed_types)\n",
        "    list_column_transformers = define_list_column_transformers(analysis_type)\n",
        "\n",
        "    # Loop through each variable and apply transformations\n",
        "    df_feat_eng = pd.DataFrame([])\n",
        "    for column in df.columns:\n",
        "        # Create duplicate columns for each transformation method\n",
        "        df_feat_eng = pd.concat([df_feat_eng, df[column]], axis=1)\n",
        "        for method in list_column_transformers:\n",
        "            df_feat_eng[f\"{column}_{method}\"] = df[column]\n",
        "\n",
        "        # Apply transformations and evaluate\n",
        "        df_feat_eng, list_applied_transformers = apply_transformers(\n",
        "            analysis_type, df_feat_eng, column)\n",
        "\n",
        "        transformer_evaluation(\n",
        "            column, list_applied_transformers, analysis_type, df_feat_eng)\n",
        "\n",
        "    return df_feat_eng\n",
        "\n",
        "\n",
        "def check_user_entry_on_analysis_type(analysis_type, allowed_types):\n",
        "    \"\"\"Validate user-specified analysis type.\"\"\"\n",
        "    if analysis_type is None:\n",
        "        raise SystemExit(\n",
        "            f\"Please pass 'analysis_type' as one of: {allowed_types}\"\n",
        "            )\n",
        "    if analysis_type not in allowed_types:\n",
        "        raise SystemExit(\n",
        "            f\"Invalid 'analysis_type'. Must be one of: {allowed_types}\"\n",
        "            )\n",
        "\n",
        "\n",
        "def check_missing_values(df):\n",
        "    \"\"\"Ensure no missing values exist before applying transformations.\"\"\"\n",
        "    if df.isna().sum().sum() != 0:\n",
        "        raise SystemExit(\n",
        "            \"Missing values detected — handle them before feature engineering.\"\n",
        "            )\n",
        "\n",
        "\n",
        "def define_list_column_transformers(analysis_type):\n",
        "    \"\"\"Return list of transformations for the given analysis type.\"\"\"\n",
        "    if analysis_type == 'numerical':\n",
        "        list_column_transformers = [\n",
        "            \"log_e\", \"log_10\", \"reciprocal\", \"power\", \"box_cox\", \"yeo_johnson\"]\n",
        "    return list_column_transformers\n",
        "\n",
        "\n",
        "def apply_transformers(analysis_type, df_feat_eng, column):\n",
        "    \"\"\"Dispatch function to apply transformations by type.\"\"\"\n",
        "    df_feat_eng, list_applied_transformers = FeatEngineering_Numerical(\n",
        "        df_feat_eng, column)\n",
        "    return df_feat_eng, list_applied_transformers\n",
        "\n",
        "\n",
        "def transformer_evaluation(column, list_applied_transformers, \n",
        "                           analysis_type, df_feat_eng):\n",
        "    \"\"\"\n",
        "    Visualize distributions and normality for original and transformed \n",
        "    variables.\n",
        "    \"\"\"\n",
        "    print(f\"* Variable Analyzed: {column}\")\n",
        "    print(f\"* Applied transformations: {list_applied_transformers}\\n\")\n",
        "    for col in [column] + list_applied_transformers:\n",
        "        DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Numerical(df, variable):\n",
        "    \"\"\"Plot histogram and Q–Q plot for a numerical variable.\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    sns.histplot(data=df, x=variable, kde=True, ax=axes[0])\n",
        "    stats.probplot(df[variable], dist=\"norm\", plot=axes[1])\n",
        "    axes[0].set_title('Histogram')\n",
        "    axes[1].set_title('Q–Q Plot')\n",
        "    fig.suptitle(f\"{variable}\", fontsize=16, y=1.05)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def FeatEngineering_Numerical(df_feat_eng, column):\n",
        "    \"\"\"\n",
        "    Apply a set of numerical transformations and track which succeed.\n",
        "    \n",
        "    Returns:\n",
        "        df_feat_eng (pd.DataFrame): Updated dataframe with successful \n",
        "        transformations.\n",
        "        list_methods_worked (list): List of successfully applied \n",
        "        transformations.\n",
        "    \"\"\"\n",
        "    list_methods_worked = []\n",
        "\n",
        "    # Log (base e)\n",
        "    try:\n",
        "        lt = vt.LogTransformer(variables=[f\"{column}_log_e\"])\n",
        "        df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_log_e\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_log_e\"], axis=1, inplace=True)\n",
        "\n",
        "    # Log (base 10)\n",
        "    try:\n",
        "        lt = vt.LogTransformer(variables=[f\"{column}_log_10\"], base='10')\n",
        "        df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_log_10\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_log_10\"], axis=1, inplace=True)\n",
        "\n",
        "    # Reciprocal\n",
        "    try:\n",
        "        rt = vt.ReciprocalTransformer(variables=[f\"{column}_reciprocal\"])\n",
        "        df_feat_eng = rt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_reciprocal\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_reciprocal\"], axis=1, inplace=True)\n",
        "\n",
        "    # Power\n",
        "    try:\n",
        "        pt = vt.PowerTransformer(variables=[f\"{column}_power\"])\n",
        "        df_feat_eng = pt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_power\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_power\"], axis=1, inplace=True)\n",
        "\n",
        "    # Box-Cox\n",
        "    try:\n",
        "        bct = vt.BoxCoxTransformer(variables=[f\"{column}_box_cox\"])\n",
        "        df_feat_eng = bct.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_box_cox\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_box_cox\"], axis=1, inplace=True)\n",
        "\n",
        "    # Yeo-Johnson\n",
        "    try:\n",
        "        yjt = vt.YeoJohnsonTransformer(variables=[f\"{column}_yeo_johnson\"])\n",
        "        df_feat_eng = yjt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_yeo_johnson\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_yeo_johnson\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Numerical Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 1: Select variables and create a separate DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "variables_engineering= (\n",
        "    TrainSet_cat\n",
        "    .select_dtypes(include=['int64', 'float64'])\n",
        "    .columns.drop(target_var).tolist()\n",
        "    )\n",
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "df_engineering.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 2: Create engineered variables by applying the encoder and assess transformation by comparing engineered variables distributions to original ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = FeatureEngineeringAnalysis(\n",
        "    df=df_engineering, analysis_type='numerical'\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Several numerical transformations were tested, including Log (base e), Log10, Reciprocal, Power, Box-Cox, and Yeo-Johnson \n",
        "* For most variables, transformations helped to make the distributions closer to normal. The chosen transformations, based on QQ plots showing values closest to the diagonal, are:\n",
        "    - Log (base e): `person_age`, `person_income`, `cb_person_cred_hist_length`  \n",
        "    - Power: `person_emp_length`, `loan_amnt`, `loan_percent_income`  \n",
        "    - None: `loan_int_rate`  \n",
        "* These transformations will be applied in the pipeline to improve feature distributions for modeling, particularly for algorithms sensitive to variable scaling and distribution shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply the selected transformation to the Train and Test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_pipeline = Pipeline([\n",
        "    ('log_transform', vt.LogTransformer(\n",
        "        variables=['person_age', 'person_income', 'cb_person_cred_hist_length'])\n",
        "     ),\n",
        "    ('power_transform', vt.PowerTransformer(\n",
        "        variables=['person_emp_length', 'loan_amnt', 'loan_percent_income'])\n",
        "     )\n",
        "])\n",
        "\n",
        "TrainSet = num_pipeline.fit_transform(TrainSet)\n",
        "TestSet = num_pipeline.transform(TestSet)\n",
        "TrainSet.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Show distributions after numerical transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pandas_report = ProfileReport(df=TrainSet[variables_engineering], minimal=True)\n",
        "pandas_report.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SmartCorrelatedSelection Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we're looking for groups of features that correlate amongst themselves,\n",
        "we want to remove any surplus correlated features since they’ll add the same information to the model.\n",
        "The transformer takes care of finding the groups and drops the features based on the method,\n",
        "threshold and selection method that we decided. This means for every group of correlated features,\n",
        "the transformer will remove all but one feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 1: Create a separate DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = TrainSet.copy()\n",
        "df_engineering.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confirm that all data types are numerical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 2: Create engineered variables applying the transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corr_sel = SmartCorrelatedSelection(variables=None, method=\"spearman\", \n",
        "                                    threshold=0.6, selection_method=\"variance\")\n",
        "\n",
        "corr_sel.fit_transform(df_engineering)\n",
        "corr_sel.correlated_feature_sets_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* After applying the `SmartCorrelatedSelection`, meaningful correlations were found between the following variable pairs:\n",
        "    - `cb_person_cred_hist_length` & `person_age`  \n",
        "    - `loan_grade` & `loan_int_rate`  \n",
        "    - `loan_amnt` & `loan_percent_income`  \n",
        "\n",
        "* From each pair, one variable will be dropped to avoid multicollinearity. These correlations were also visible in the PPS heatmap during the exploratory data analysis  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corr_sel.features_to_drop_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* No other features were flagged for removal, confirming that overall multicollinearity is limited. Therefore, we will include the `SmartCorrelatedSelection` step in our pipeline to automatically handle these high correlations during preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply the SmartCorrelatedSelection to the Train and Test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corr_sel = SmartCorrelatedSelection(variables=None, method=\"spearman\", \n",
        "                                    threshold=0.6, selection_method=\"variance\")\n",
        "\n",
        "TrainSet = corr_sel.fit_transform(TrainSet)\n",
        "TestSet = corr_sel.transform(TestSet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scaling\n",
        "\n",
        "We apply scaling to the dataset so that all variables are on a comparable range. This is important because many machine learning algorithms are sensitive to the magnitude of features. Without scaling, variables with larger ranges could dominate the learning process, leading to biased or suboptimal models.\n",
        "\n",
        "Note that after scaling, especially the categorical values are standardized and lose their original interpretability, but this ensures that all numerical inputs contribute comparably to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = TrainSet.copy()\n",
        "scaler = StandardScaler()\n",
        "scaled_array  = scaler.fit_transform(df_engineering)\n",
        "df_engineering = pd.DataFrame(scaled_array, columns=df_engineering.columns)\n",
        "df_engineering.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After applying the StandardScaler, all variables have a mean of 0 and a standard deviation of 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering.describe().round(2).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply StandardScaler to the Train and Test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "TrainSet = scaler.fit_transform(TrainSet)\n",
        "TestSet = scaler.transform(TestSet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Conclusions and Next Steps\n",
        "\n",
        "Feature Engineering Steps we will apply:\n",
        "1. Categorical variables\n",
        "    * Nominal Variables: OrdinalEncoder (arbitrary)  \n",
        "        - `person_home_ownership`, `loan_intent`, `cb_person_default_on_file`  \n",
        "    * Ordinal Variables: OrdinalEncoder (ordered)  \n",
        "        - `loan_grade`\n",
        "2. Numerical variables   \n",
        "    - Log (base e): `person_age`, `person_income`, `cb_person_cred_hist_length`  \n",
        "    - Power: `person_emp_length`, `loan_amnt`, `loan_percent_income`  \n",
        "    - None: `loan_int_rate` \n",
        "3. `SmartCorrelatedSelection` will remove one variable from each highly correlated pair to reduce multicollinearity \n",
        "2. Scaling variables\n",
        "\n",
        "Next Steps:\n",
        "* Model loan defaults and evaluate model performance\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
