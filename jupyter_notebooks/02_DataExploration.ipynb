{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# Data Exploration Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Perform univariate, bivariate and correlation analyses to explore the dataset’s structure, identify key relationships between variables and generate insights relevant to Business Requirement 1\n",
        "    * Business Requirement 1: Data Insights (Conventional Analysis)  \n",
        "        Identify key customer and loan attributes that are most correlated with loan default. Provide visual and statistical insights to help business analysts understand the primary drivers of credit risk.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/LoanDefaultData.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Generate code that answers Business Requirement 1 and can be used within the Streamlit App\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# for vs code\n",
        "%matplotlib inline \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import ppscore as pps\n",
        "from ydata_profiling import ProfileReport\n",
        "# Ignore FutureWarnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder, where the notebook is stored, to its parent folder\n",
        "* First we access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "* Then we want to make the parent of the current directory the new current directory\n",
        "    * os.path.dirname() gets the parent directory\n",
        "    * os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "current_dir = os.getcwd()\n",
        "print(f\"You set a new current directory: {current_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"outputs/datasets/collection/LoanDefaultData.csv\")\n",
        "    \n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set the target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_var = \"loan_status\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Univariate Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section we examine each variable individually, understand its distribution and check for missing levels. This helps us get a clear overview of the dataset before moving on to explore the relationships between variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert object columns to categorical so that it can be displayed \n",
        "# properly in the report\n",
        "df_cat = df.copy()\n",
        "for col in df_cat.select_dtypes(include='object').columns:\n",
        "    df_cat[col] = df_cat[col].astype('category')\n",
        "\n",
        "# Also transform target variable to categorical\n",
        "df_cat[target_var] = df_cat[target_var].astype('category')\n",
        "    \n",
        "pandas_report = ProfileReport(df=df_cat, minimal=True)\n",
        "pandas_report.to_notebook_iframe() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The profile report confirmes that missing values exist only in `person_emp_length` and `loan_int_rate`\n",
        "* It also shows that numerical features exhibit right-skewed distributions with long tails, and that most of the outliers are concentrated on the higher-value end of each variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To make key observations easier to digest, additional univariate analyses were performed, highlighting distributions, skewness/kurtosis and class balance for both numerical and categorical features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Distribution Analysis of Numerical Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_cols = df_cat.select_dtypes(include=np.number).columns\n",
        "\n",
        "summary = df_cat[num_cols].describe().T\n",
        "summary['skewness'] = df_cat[num_cols].skew()\n",
        "summary['kurtosis'] = df_cat[num_cols].kurtosis()\n",
        "summary = summary.round(2)\n",
        "\n",
        "summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The numerical variables in the dataset generally display **right-skewed distributions with long tails**, as reflected by positive skewness values. Kurtosis values vary across features, with some indicating heavier tails (e.g. `person_income`, `person_age`, and `person_emp_length`), suggesting the presence of outliers, particularly at the higher end of the distributions.  \n",
        "* Comparing the median values between features shows substantial differences in scale, ranging from over 55,000 for `person_income` to 0.15 for `loan_percent_income`. This highlights the need for **feature scaling** before modeling, as many machine learning algorithms are sensitive to differences in magnitude and may otherwise assign disproportionate importance to larger-scale variables.\n",
        "\n",
        "The following boxplots confirm these observations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grid layout for plots\n",
        "numeric_cols = df_cat.select_dtypes(include=['float64', 'int64']).columns\n",
        "n_cols = 2\n",
        "n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*5, n_rows*2))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(numeric_cols):\n",
        "    sns.boxplot(x=df_cat[col], ax=axes[i], color=sns.color_palette(\"Set2\")[0])\n",
        "    axes[i].set_title(f\"{col}\")\n",
        "\n",
        "# Remove any unused subplots\n",
        "for j in range(i+1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* In addition to the previous observations, the boxplots reveal that all numerical variables contain outliers, with the most pronounced extreme values observed in `person_income`, `person_age`, and `person_emp_length`, consistent with the skewness and kurtosis analysis conducted earlier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Distribution Analysis of Categorical Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_cols = df_cat.select_dtypes(include='category').columns.tolist()\n",
        "\n",
        "# Grid layout for plots\n",
        "n_cols = 2\n",
        "n_rows = (len(cat_cols) + n_cols - 1) // n_cols\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*5, n_rows*4))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(cat_cols):\n",
        "    sns.countplot(x=col, data=df_cat, ax=axes[i], hue=col, palette='Set2')\n",
        "    axes[i].set_title(f\"{col} Distribution\")\n",
        "    axes[i].set_xlabel('')\n",
        "    axes[i].set_ylabel('Count')\n",
        "    axes[i].tick_params(axis='x', rotation=90)\n",
        "\n",
        "# Remove empty subplots\n",
        "for j in range(i+1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The categorical variables in the dataset contain only a small number of distinct classes. This indicates that the features are not overly granular, making them suitable for analysis and modeling.  \n",
        "* Still some features contain categories with very few observations. These rare categories might have to be addressed during preprocessing. Tree-based models such as Random Forest or XGBoost can generally handle rare categories and imbalances naturally, while linear or logistic models may require explicit handling to avoid unstable coefficients or overfitting.\n",
        "* We can also see that the target variable `loan_status` is imbalanced towards 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Overall, the distribution analysis shows that the dataset is generally suitable for modeling, but some considerations are required during preprocessing:  \n",
        "* **Numerical features** display right-skewed distributions with long tails, and several variables (e.g. `person_income`, `person_age`, `person_emp_length`) contain high-value outliers. Feature scales vary widely, highlighting the need for feature scaling before modeling\n",
        "* **Categorical features** have a limited number of distinct classes, making them easy to encode. Some categories have very few observations, which may require combining or encoding strategies depending on the model; tree-based models can generally handle these naturally, while linear models may need explicit handling\n",
        "* **Target variable** `loan_status` is imbalanced towards 0, which should be addressed during model development to ensure reliable predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bivariate Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After examining the univariate distributions to understand each feature individually, the next step is to explore how variables relate to the target variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Distribution Analysis of Numerical Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numeric_cols = df_cat.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "n_cols = 3  \n",
        "n_rows = (len(numeric_cols) + n_cols - 1) // n_cols \n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*5, n_rows*3))\n",
        "axes = axes.flatten()  \n",
        "\n",
        "# Define the numeric columns with large outliers to filter for outliers  \n",
        "# to have a better visualisation\n",
        "outlier_cols = ['person_income', 'person_age', 'person_emp_length']\n",
        "\n",
        "for i, col in enumerate(numeric_cols):\n",
        "    # Filter out outliers only for the specified columns\n",
        "    if col in outlier_cols:\n",
        "        upper = df_cat[col].quantile(0.99)\n",
        "        filtered_df = df_cat[df_cat[col] <= upper]\n",
        "    else:\n",
        "        filtered_df = df_cat.copy()\n",
        "        \n",
        "    sns.histplot(\n",
        "        data=filtered_df, \n",
        "        x=col, \n",
        "        hue=target_var, \n",
        "        ax=axes[i], \n",
        "        kde=True, \n",
        "        palette=\"Set2\")\n",
        "    axes[i].set_title(f\"{col}\")\n",
        "\n",
        "# Remove any unused subplots\n",
        "for j in range(i+1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Distribution Analysis of Categorical Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_cols = (df_cat\n",
        "            .select_dtypes(include='category')\n",
        "            .columns\n",
        "            .drop(target_var)\n",
        "            .tolist())\n",
        "\n",
        "n_cols = 2\n",
        "n_rows = (len(cat_cols) + n_cols - 1) // n_cols\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*5, n_rows*4))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(cat_cols):\n",
        "    sns.countplot(x=col, data=df_cat, ax=axes[i], \n",
        "                  hue=target_var, palette='Set2')\n",
        "    axes[i].set_title(f\"{col} Distribution\")\n",
        "    axes[i].set_xlabel('')\n",
        "    axes[i].set_ylabel('Count')\n",
        "    axes[i].tick_params(axis='x', rotation=90)\n",
        "\n",
        "# Remove empty subplots\n",
        "for j in range(i+1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The bivariate analysis revealed that several numerical variables, such as `person_income`, `loan_int_rate`, and `person_emp_length`, show clear positive or negative relationships with `loan_status`. This suggests that these features may have some predictive influence on the target variable\n",
        "* In general, categorical variables display weaker and less consistent patterns, indicating that their effect on `loan_status` is less direct or may depend on interactions with other variables. However, some trends are visible. For example, applicants who **rent their homes** seem more likely to default compared to those who own or have a mortgage. Also `loan_grade` shows a relationship to `loan_status`, with lower loan grades having more defaults than no defaults.\n",
        "\n",
        "Next, we explore correlation analyses to quantify these relationships and further understand how the features relate to each other and to the target variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correlation Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Correlation analysis helps identify linear or non-linear relationships between features and with the target variable.\n",
        "\n",
        "This step provides insights into potential multicollinearity among predictors and highlights which variables may have predictive influence on loan default. We will also use these insights to answer Business Requirement 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To explore how features relate to each other and to the target variable, we use three complementary measures:\n",
        "\n",
        "* Pearson correlation: captures linear relationships\n",
        "* Spearman correlation: captures monotonic (rank-based) relationships\n",
        "* Power Predictive Score (PPS): detects non-linear and directional predictive relationships\n",
        "\n",
        "Since each method provides a different perspective, comparing all three helps ensure we capture both linear and complex dependencies in the data.\n",
        "\n",
        "The helper functions below automate this process:\n",
        "\n",
        "* heatmap_corr() → Plots correlation matrices (Pearson or Spearman) masking for low-correlation values\n",
        "* heatmap_pps() → Plots the PPS matrix masking for low-correlation values\n",
        "* CalculateCorrAndPPS() → Calculates Pearson, Spearman and PPS matrices and prints descriptive statistics for PPS to help select masking thresholds\n",
        "* DisplayCorrAndPPS() → Combines all plots in one view to facilitate interpretation and comparison\n",
        "\n",
        "Together, these functions provide a structured way to assess relationships among variables and evaluate which features may be most predictive of loan default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# From Churnometer Walkthrough Project\n",
        "def heatmap_corr(df, threshold, figsize=(20, 12), font_annot=8):\n",
        "    if len(df.columns) > 1:\n",
        "        mask = np.zeros_like(df, dtype=bool)\n",
        "        mask[np.triu_indices_from(mask)] = True\n",
        "        mask[abs(df) < threshold] = True\n",
        "\n",
        "        fig, axes = plt.subplots(figsize=figsize)\n",
        "        sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
        "                    mask=mask, cmap='viridis', annot_kws={\"size\": font_annot}, \n",
        "                    ax=axes, linewidth=0.5\n",
        "                    )\n",
        "        axes.set_yticklabels(df.columns, rotation=0)\n",
        "        plt.ylim(len(df.columns), 0)\n",
        "        plt.show()\n",
        "\n",
        "def heatmap_pps(df, threshold, figsize=(20, 12), font_annot=8):\n",
        "    if len(df.columns) > 1:\n",
        "        mask = np.zeros_like(df, dtype=bool)\n",
        "        mask[abs(df) < threshold] = True\n",
        "        fig, ax = plt.subplots(figsize=figsize)\n",
        "        ax = sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
        "                         mask=mask, cmap='rocket_r', \n",
        "                         annot_kws={\"size\": font_annot},\n",
        "                         linewidth=0.05, linecolor='grey')\n",
        "        plt.ylim(len(df.columns), 0)\n",
        "        plt.ylabel(\"\")\n",
        "        plt.xlabel(\"\")\n",
        "        plt.show()\n",
        "\n",
        "def CalculateCorrAndPPS(df):\n",
        "    df_corr_spearman = df.corr(method=\"spearman\", numeric_only=True)\n",
        "    df_corr_pearson = df.corr(method=\"pearson\", numeric_only=True)\n",
        "\n",
        "    pps_matrix_raw = pps.matrix(df)\n",
        "    pps_matrix = (pps_matrix_raw\n",
        "                  .filter(['x', 'y', 'ppscore'])\n",
        "                  .pivot(columns='y', index='x', values='ppscore'))\n",
        "\n",
        "    pps_score_stats = (pps_matrix_raw\n",
        "                       .query(\"ppscore < 1\")\n",
        "                       .filter(['ppscore'])\n",
        "                       .describe().T)\n",
        "    print(\n",
        "        \"PPS threshold - check PPS score IQR \"\n",
        "        \"to decide threshold for heatmap\\n\"\n",
        "    )\n",
        "    print(pps_score_stats.round(3))\n",
        "\n",
        "    return df_corr_pearson, df_corr_spearman, pps_matrix\n",
        "\n",
        "def DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, \n",
        "                      pps_matrix, CorrThreshold, PPS_Threshold,\n",
        "                      figsize=(20, 12), font_annot=8):\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\n",
        "        \"* Analyse how the target variable is correlated with \"\n",
        "        \"other variables (features and target)\"\n",
        "        )\n",
        "    print(\n",
        "        \"* Analyse multi-collinearity, that is, how the features \"\n",
        "        \"are correlated among themselves\")\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Spearman Correlation ***\")\n",
        "    print(\"Evaluates monotonic relationships (continuous variables)\\n\")\n",
        "    heatmap_corr(df=df_corr_spearman, threshold=CorrThreshold, \n",
        "                 figsize=figsize, font_annot=font_annot)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Pearson Correlation ***\")\n",
        "    print(\"Evaluates linear relationships (continuous variables)\\n\")\n",
        "    heatmap_corr(df=df_corr_pearson, threshold=CorrThreshold, \n",
        "                 figsize=figsize, font_annot=font_annot)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
        "    print(\"PPS detects linear or non-linear relationships between \"\n",
        "          \"two variables\\n\"\n",
        "          \"The score ranges from 0 (no predictive power) to \"\n",
        "          \"1 (perfect predictive power) \\n\")\n",
        "    heatmap_pps(df=pps_matrix, threshold=PPS_Threshold, \n",
        "                figsize=figsize, font_annot=font_annot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Transform target variable back to integer so that it can be used in pearson and spearman correlation analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cat[target_var] = df_cat[target_var].astype('int')\n",
        "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(df_cat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The PPS (Predictive Power Score) analysis indicates that most variable pairs have **very low predictive power**, with a median and 75th percentile of 0 and 0.005 respectively, and a mean of 0.042. Only a few pairs reach higher values (up to 0.956), suggesting limited but notable predictive potential, but also possible multicollinearity.  \n",
        "\n",
        "Based on the PPS distribution and its interquartile range, a **threshold of 0.04** was chosen to filter out pairs with effectively zero predictive power while retaining those with even minimal above-average potential for inclusion in the heatmap.\n",
        "\n",
        "Analogously, for the Pearson and Spearman correlation heatmaps, a threshold of 0.2 was selected to capture the available correlations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CorrThreshold = 0.2\n",
        "PPS_Threshold = 0.04\n",
        "DisplayCorrAndPPS(df_corr_pearson = df_corr_pearson,\n",
        "                  df_corr_spearman = df_corr_spearman, \n",
        "                  pps_matrix = pps_matrix,\n",
        "                  CorrThreshold = CorrThreshold, PPS_Threshold = PPS_Threshold,\n",
        "                  figsize=(15,10), font_annot=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The correlation and PPS heatmaps indicate that correlations with the target variable `loan_status` remain generally low, suggesting that no single predictor strongly explains default behavior \n",
        "\n",
        "* However, several predictors show **high multicollinearity**, including:  \n",
        "    - `person_age` and `cb_person_cred_hist_length`  \n",
        "    - `loan_percent_income` and `loan_amnt`  \n",
        "    - `loan_grade` and `loan_int_rate`\n",
        "\n",
        "    These relationships are consistent with business expectations - for example, older applicants tend to have longer credit histories, larger loans take up a larger amount of peoples income, and worse loan grades are associated with higher interest rates. \n",
        "\n",
        "* Overall, while multicollinearity exists among some features, it reflects logical, interpretable relationships rather than data issues, and the default outcome is likely influenced by a combination of factors\n",
        "\n",
        "\n",
        "The variables that showed some predictive power on the target variable are listed below, ordered by their PPS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect variables with some predictive signal from PPS and correlations\n",
        "pps_vars = (\n",
        "    pps_matrix.T[target_var]\n",
        "    [abs(pps_matrix.T[target_var]) > PPS_Threshold]\n",
        "    .drop(target_var).index)\n",
        "pearson_vars = (\n",
        "    df_corr_pearson[target_var]\n",
        "    [abs(df_corr_pearson[target_var]) > CorrThreshold]\n",
        "    .drop(target_var).index)\n",
        "spearman_vars = (\n",
        "    df_corr_spearman[target_var]\n",
        "    [abs(df_corr_pearson[target_var]) > CorrThreshold]\n",
        "    .drop(target_var).index)\n",
        "\n",
        "# Combine all and remove duplicates\n",
        "focus_vars = pd.Index(pps_vars).union(pearson_vars).union(spearman_vars)\n",
        "\n",
        "# Sort by absolute PPS\n",
        "focus_vars = pps_matrix.T[target_var][focus_vars].sort_values(key=abs, \n",
        "                                                              ascending=False)\n",
        "print(\"The features with highest predictive power, ordered by PPS:\")\n",
        "print(focus_vars)\n",
        "\n",
        "focus_vars = focus_vars.index.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Overall the correlation analysis confirms the patterns observed in the bivariate analysis\n",
        "\n",
        "Next we visualize how the identified variables relate to the target. This complements the correlation analysis by showing the actual distributions and highlighting patterns or imbalances that may influence predictive modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_var = 'loan_status'\n",
        "\n",
        "for col in focus_vars:\n",
        "    if df_cat[col].dtype in ['int64', 'float64']:\n",
        "        # Create a figure with two stacked axes (boxplot above histogram)\n",
        "        # Source: https://python-graph-gallery.com/24-histogram-with-a-boxplot-on-top-seaborn/\n",
        "        fig, (ax_box, ax_hist) = plt.subplots(\n",
        "            2, 1, \n",
        "            figsize=(8, 4), \n",
        "            gridspec_kw={\"height_ratios\": [0.25, 1], \"hspace\": 0.05}\n",
        "        )\n",
        "\n",
        "        # Boxplot (small on top)\n",
        "        sns.boxplot(data=df_cat, x=col, y=target_var, hue=target_var, \n",
        "                    orient=\"h\", ax=ax_box, palette=\"Set2\", legend=False)\n",
        "        ax_box.set(xlabel=\"\")  \n",
        "        ax_box.set_ylabel(\"\")  \n",
        "        ax_box.set_title(f\"{col} distribution by {target_var}\", fontsize=12)\n",
        "\n",
        "        # Histogram (main plot)\n",
        "        sns.histplot(data=df_cat, x=col, hue=target_var, kde=True, \n",
        "                     ax=ax_hist, palette=\"Set2\")\n",
        "        ax_hist.set_xlabel(col)\n",
        "        ax_hist.set_ylabel(\"Count\")\n",
        "        ax_hist.set_title(\"\")\n",
        "    \n",
        "    else:\n",
        "        sns.countplot(data=df_cat, x=col, hue=target_var, palette=\"Set2\", \n",
        "                      order=df[col].value_counts().index)\n",
        "        plt.xlabel(\"\")\n",
        "        plt.ylabel(\"\")\n",
        "        plt.title(f\"{col} distribution by {target_var}\", fontsize=12)\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Borrowers who default tend to show the following general trends:\n",
        "    * Pay rent\n",
        "    * Have higher interest rates\n",
        "    * Have a higher loan amount relative to their income\n",
        "* This doesn't mean all defaulting customers have all these patterns at the same time, we just noticed a general trend\n",
        "\n",
        "> NOTE: Interest rates are strongly correlated with the loan grade. From a business perspective, having a lower/worse loan grade implies, that a borrower will have worse loan terms including higher interest rates. Therefore, having a higher interest rate directly implies that the borrower is more risky."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parallel Plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After identifying the features that are most correlated with the target, the following parallel categories plot helps explore how these variables interact together to influence default outcomes.\n",
        "While correlation analysis shows individual relationships, this plot reveals combined patterns and interactions that may drive borrower behavior, offering a more complete view of default risk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This plot type needs categorical variables. To keep it simple we bin the numerical variables into equally sized groups ranging from low to very high."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_parallel = df_cat.copy()[[\"loan_status\"] + focus_vars]\n",
        "\n",
        "# Discretize numerical features into quartile bins\n",
        "df_parallel['loan_int_rate'] = pd.qcut(\n",
        "    df_parallel['loan_int_rate'], \n",
        "    q=4,\n",
        "    labels=['Low', 'Medium', 'High', 'Very High'])\n",
        "\n",
        "df_parallel['loan_percent_income'] = pd.qcut(\n",
        "    df_parallel['loan_percent_income'],\n",
        "    q=4,\n",
        "    labels=['Low', 'Medium', 'High', 'Very High'])\n",
        "\n",
        "df_parallel.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the parallel plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = px.parallel_categories(df_parallel, color=\"loan_status\",\n",
        "                             color_continuous_scale=px.colors.diverging.RdYlGn)\n",
        "fig.show(renderer='jupyterlab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The parallel categories plot shows the same general patterns observed in the previous section, but now combined in a single visualization.\n",
        "It highlights how multiple borrower and loan attributes interact with each other and with default status.\n",
        "We can see that higher loan amounts relative to income, higher interest rates,and paying rent tend to be associated with increased default probability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Conclusions and Next Steps\n",
        "\n",
        "The univariate and correlation analyses provided an initial understanding of the data distribution and key variable relationships with the target.\n",
        "\n",
        "In the univariate analysis, numerical variables generally display **right-skewed distributions with long tails**, and features such as `person_income`, `person_age`, and `person_emp_length` contain high-value outliers. The scales of numerical features vary widely, indicating the need for **feature scaling**. Categorical variables have a limited number of distinct classes, although some categories have very few observations, which may require handling depending on the model choice. The target variable is imbalanced towards 0.\n",
        "\n",
        "Bivariate analysis suggests that numerical features such as `person_income`, `loan_int_rate`, and `person_emp_length` show noticeable relationships with the target, while categorical features exhibit weaker patterns, with trends like borrowers who pay rent being more likely to default.\n",
        "\n",
        "Correlation and PPS analyses reveal that correlations with the target are generally low, but some predictors show high multicollinearity. These relationships are consistent with business logic and indicate that default behavior is influenced by a combination of factors rather than a single feature.\n",
        "\n",
        "The analysis showed that borrowers who default tend to show the following general trends:\n",
        "* Pay rent\n",
        "* Have higher interest rates (higher risk)\n",
        "* Have a higher loan amount relative to their income\n",
        "\n",
        "Next Steps:\n",
        "* Conduct data cleaning activities"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
