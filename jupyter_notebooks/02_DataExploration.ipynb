{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Perform univariate and correlation analyses to explore the dataset’s structure, identify key relationships between variables and generate insights relevant to Business Requirement 1\n",
        "    * Business Requirement 1: Data Insights (Conventional Analysis)  \n",
        "        Identify key customer and loan attributes that are most correlated with loan default. Provide visual and statistical insights to help business analysts understand the primary drivers of credit risk.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/LoanDefaultData.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Generate code that answers Business Requirement 1 and can be used within the Streamlit App\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ignore FutureWarnings for message \"is_categorical_dtype is deprecated\"\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pandas.core.dtypes.common\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder, where the notebook is stored, to its parent folder\n",
        "* First we access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "* Then we want to make the parent of the current directory the new current directory\n",
        "    * os.path.dirname() gets the parent directory\n",
        "    * os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "current_dir = os.getcwd()\n",
        "print(f\"You set a new current directory: {current_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As the variable `LoanID` is a unique identifier for each record, it does not contribute to the prediction and will therefore be dropped for the following analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = (pd.read_csv(\"outputs/datasets/collection/LoanDefaultData.csv\")\n",
        "    .drop(['LoanID'], axis=1)\n",
        "    )\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Exploration\n",
        "\n",
        "we will check univariate and multivariate analysis -> todo describe section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Univariate Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section we examine each variable individually, understand its distribution and check for missing values. This helps us get a clear overview of the dataset before moving on to relationships between variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "# Convert object columns to categorical so that it can be displayed properly in the report\n",
        "df_cat = df.copy()\n",
        "for col in df_cat.select_dtypes(include='object').columns:\n",
        "    df_cat[col] = df_cat[col].astype('category')\n",
        "\n",
        "# Also transform tagret variable to categorical\n",
        "df_cat[\"Default\"] = df_cat[\"Default\"].astype('category')\n",
        "    \n",
        "pandas_report = ProfileReport(df=df_cat, minimal=True)\n",
        "pandas_report.to_notebook_iframe() # needs: pip --upgrade setuptools and pip install notebook ipython==8.24.0  ipykernel ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The profile report confirmed that the dataset contains no missing values\n",
        "* Additionally, it shows that the variables `NumCreditLines` and `LoanTerm` are rather categorical variables than continuous numerical variables, as they have only a limited number of different values. They will be transformed for the following analyses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for col in [\"NumCreditLines\", \"LoanTerm\"]:\n",
        "    df_cat[col] = df_cat[col].astype('category')\n",
        "\n",
        "df_cat.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To make key observations easier to digest, additional univariate analyses were performed, highlighting distributions, skewness/kurtosis and class balance for both numerical and categorical features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Distribution Analysis of Numerical Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Select numerical columns\n",
        "num_cols = df_cat.select_dtypes(include=np.number).columns\n",
        "\n",
        "# Create summary dataframe\n",
        "summary = df_cat[num_cols].describe().T  # gives count, mean, std, min, 25%, 50%, 75%, max\n",
        "\n",
        "# Add skewness\n",
        "summary['skewness'] = df_cat[num_cols].skew()\n",
        "\n",
        "# Optionally, add kurtosis\n",
        "summary['kurtosis'] = df_cat[num_cols].kurtosis()\n",
        "\n",
        "# Round for better readability\n",
        "summary = summary.round(2)\n",
        "\n",
        "summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The numerical variables in the dataset show a generally well-balanced distribution pattern. Most features (such as Age, Income, LoanAmount, and CreditScore) have skewness values close to 0 and slightly negative kurtosis values around −1.2, indicating approximately symmetric distributions with light tails. This suggests that the dataset does not contain extreme outliers or heavy skewness that would require major transformations (e.g., log or Box-Cox scaling).\n",
        "\n",
        "The following boxplots confirm these observations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "numeric_cols = df_cat.select_dtypes(include=['float64', 'int64']).columns\n",
        "n_cols = 2  # number of plots per row\n",
        "n_rows = (len(numeric_cols) + n_cols - 1) // n_cols  # calculate rows needed\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*5, n_rows*2))\n",
        "axes = axes.flatten()  # flatten 2D array for easy indexing\n",
        "\n",
        "for i, col in enumerate(numeric_cols):\n",
        "    sns.boxplot(x=df_cat[col], ax=axes[i], palette=\"Set2\", hue=df_cat[col], legend=False)\n",
        "    axes[i].set_title(f\"{col}\")\n",
        "\n",
        "# Remove any unused subplots\n",
        "for j in range(i+1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Distribution Analysis of Categorical Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select categorical columns\n",
        "cat_cols = df_cat.select_dtypes(include='category').columns.tolist()\n",
        "\n",
        "# Grid layout for plots\n",
        "n_cols = 3\n",
        "n_rows = (len(cat_cols) + n_cols - 1) // n_cols\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*5, n_rows*3))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(cat_cols):\n",
        "    sns.countplot(x=col, data=df_cat, ax=axes[i], hue=col, palette='Set2')\n",
        "    axes[i].set_title(f\"{col} Distribution\")\n",
        "    axes[i].set_xlabel('')\n",
        "    axes[i].set_ylabel('Count')\n",
        "\n",
        "# Remove empty subplots\n",
        "for j in range(i+1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The categorical variables in the dataset are generally well balanced, with each feature containing only 2–5 distinct classes. This indicates that the features are not overly granular, making them suitable for analysis and modeling. \n",
        "* The exception is the target variable `Default`, which is imbalanced toward 0 (non-default), reflecting that the majority of borrowers in the dataset did not default, what could already be seen in the \"Data Collection\" Notebook in the \"Target Variable Exploration\" section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Overall, the distribution analysis confirms that:\n",
        "* The numerical as well as categorical predictors are well-balanced and suitable for further modeling without extensive preprocessing\n",
        "* The target imbalance in `Default` should be addressed later during model development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correlation Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After examining the univariate distributions to understand each feature individually, the next step is to explore how variables relate to one another.\n",
        "Correlation analysis helps identify linear or non-linear relationships between features and with the target variable.\n",
        "\n",
        "This step provides insights into potential multicollinearity among predictors and highlights which variables may have predictive influence on loan default.\n",
        "Understanding these relationships is essential for guiding feature selection, hypothesis validation and the design of machine learning models later in the project. We will also use these insights to answer Business Requirement 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To explore how features relate to each other and to the target variable (Default), we use three complementary measures:\n",
        "\n",
        "* Pearson correlation: captures linear relationships\n",
        "* Spearman correlation: captures monotonic (rank-based) relationships\n",
        "* Power Predictive Score (PPS): detects non-linear and directional predictive relationships\n",
        "\n",
        "Since each method provides a different perspective, comparing all three helps ensure we capture both linear and complex dependencies in the data.\n",
        "\n",
        "The helper functions below automate this process:\n",
        "\n",
        "* heatmap_corr() → Plots correlation matrices (Pearson or Spearman) masking for low-correlation values\n",
        "* heatmap_pps() → Plots the PPS matrix masking for low-correlation values\n",
        "* CalculateCorrAndPPS() → Calculates Pearson, Spearman and PPS matrices and prints descriptive statistics for PPS to help select masking thresholds\n",
        "* DisplayCorrAndPPS() → Combines all plots in one view to facilitate interpretation and comparison\n",
        "\n",
        "Together, these functions provide a structured way to assess relationships among variables and evaluate which features may be most predictive of loan default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# From Churnometer Walkthrough Project\n",
        "import numpy as np\n",
        "# for vs code\n",
        "%matplotlib inline \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ppscore as pps\n",
        "\n",
        "\n",
        "def heatmap_corr(df, threshold, figsize=(20, 12), font_annot=8):\n",
        "    if len(df.columns) > 1:\n",
        "        mask = np.zeros_like(df, dtype=bool)\n",
        "        mask[np.triu_indices_from(mask)] = True\n",
        "        mask[abs(df) < threshold] = True\n",
        "\n",
        "        fig, axes = plt.subplots(figsize=figsize)\n",
        "        sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
        "                    mask=mask, cmap='Set2', annot_kws={\"size\": font_annot}, ax=axes,\n",
        "                    linewidth=0.5\n",
        "                    )\n",
        "        axes.set_yticklabels(df.columns, rotation=0)\n",
        "        plt.ylim(len(df.columns), 0)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def heatmap_pps(df, threshold, figsize=(20, 12), font_annot=8):\n",
        "    if len(df.columns) > 1:\n",
        "        mask = np.zeros_like(df, dtype=bool)\n",
        "        mask[abs(df) < threshold] = True\n",
        "        fig, ax = plt.subplots(figsize=figsize)\n",
        "        ax = sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
        "                         mask=mask, cmap='Set2', annot_kws={\"size\": font_annot},\n",
        "                         linewidth=0.05, linecolor='grey')\n",
        "        plt.ylim(len(df.columns), 0)\n",
        "        plt.ylabel(\"\")\n",
        "        plt.xlabel(\"\")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def CalculateCorrAndPPS(df):\n",
        "    df_corr_spearman = df.corr(method=\"spearman\", numeric_only=True)\n",
        "    df_corr_pearson = df.corr(method=\"pearson\", numeric_only=True)\n",
        "\n",
        "    pps_matrix_raw = pps.matrix(df)\n",
        "    pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(columns='y', index='x', values='ppscore')\n",
        "\n",
        "    pps_score_stats = pps_matrix_raw.query(\"ppscore < 1\").filter(['ppscore']).describe().T\n",
        "    print(\"PPS threshold - check PPS score IQR to decide threshold for heatmap \\n\")\n",
        "    print(pps_score_stats.round(3))\n",
        "\n",
        "    return df_corr_pearson, df_corr_spearman, pps_matrix\n",
        "\n",
        "\n",
        "def DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix, CorrThreshold, PPS_Threshold,\n",
        "                      figsize=(20, 12), font_annot=8):\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"* Analyse how the target variable is correlated with other variables (features and target)\")\n",
        "    print(\"* Analyse multi-collinearity, that is, how the features are correlated among themselves\")\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Spearman Correlation ***\")\n",
        "    print(\"Evaluates monotonic relationships (continuous variables)\\n\")\n",
        "    heatmap_corr(df=df_corr_spearman, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Pearson Correlation ***\")\n",
        "    print(\"Evaluates linear relationships (continuous variables)\\n\")\n",
        "    heatmap_corr(df=df_corr_pearson, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
        "    print(f\"PPS detects linear or non-linear relationships between two variables\\n\"\n",
        "          f\"The score ranges from 0 (no predictive power) to 1 (perfect predictive power) \\n\")\n",
        "    heatmap_pps(df=pps_matrix, threshold=PPS_Threshold, figsize=figsize, font_annot=font_annot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# transform target back to integer so that it can be used in pearson and spearman correlation analysis\n",
        "df_cat[\"Default\"] = df_cat[\"Default\"].astype('int')\n",
        "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(df_cat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The PPS summary shows that most variable pairs have no correlation, while a few reach slightly higher values (up to 0.023), indicating some predictive potential. \n",
        "\n",
        "Based on this, a PPS threshold of 0.001 was chosen to filter out zero relationships while highlighting pairs with above-average minimal predictive power. \n",
        "\n",
        "Analogously, for the Pearson and Spearman correlation heatmaps, a threshold of 0.01 was selected to capture the minimal available correlations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CorrThreshold = 0.02\n",
        "PPS_Threshold = 0.001\n",
        "DisplayCorrAndPPS(df_corr_pearson = df_corr_pearson,\n",
        "                  df_corr_spearman = df_corr_spearman, \n",
        "                  pps_matrix = pps_matrix,\n",
        "                  CorrThreshold = CorrThreshold, PPS_Threshold = PPS_Threshold,\n",
        "                  figsize=(15,10), font_annot=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The correlation and PPS heatmaps show generally low correlations among predictors and with the `Default` variable. \n",
        "This indicates that multicollinearity is not a major concern, but also that no single variable strongly explains loan default. \n",
        "Instead, default behavior is likely influenced by a combination of factors.\n",
        "* The variables that showed at least some predictive power on the target variable are listed below, ordered by their Pearson correlation with the target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect variables with some predictive signal from PPS and correlations\n",
        "pps_vars = pps_matrix.T[\"Default\"][abs(pps_matrix[\"Default\"]) > PPS_Threshold].drop(\"Default\").index\n",
        "pearson_vars = df_corr_pearson[\"Default\"][abs(df_corr_pearson[\"Default\"]) > CorrThreshold].drop(\"Default\").index\n",
        "spearman_vars = df_corr_spearman[\"Default\"][abs(df_corr_pearson[\"Default\"]) > CorrThreshold].drop(\"Default\").index\n",
        "\n",
        "# Combine all and remove duplicates\n",
        "focus_vars = pd.Index(pps_vars).union(pearson_vars).union(spearman_vars)\n",
        "\n",
        "# Sort by absolute Pearson correlation with Default\n",
        "focus_vars = df_corr_pearson[\"Default\"][focus_vars].sort_values(key=abs, ascending=False)\n",
        "print(\"The feature with highest predictive power, ordered by absolute Pearson correlation:\")\n",
        "print(focus_vars)\n",
        "\n",
        "focus_vars = focus_vars.index.tolist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we visualize how the identified variables relate to the target. This complements the correlation analysis by showing the actual distributions and highlighting patterns or imbalances that may influence predictive modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "target_var = 'Default'\n",
        "\n",
        "for col in focus_vars:\n",
        "    # Create a figure with two stacked axes (boxplot above histogram)\n",
        "    # Source: https://python-graph-gallery.com/24-histogram-with-a-boxplot-on-top-seaborn/\n",
        "    fig, (ax_box, ax_hist) = plt.subplots(\n",
        "        2, 1, \n",
        "        figsize=(8, 4), \n",
        "        gridspec_kw={\"height_ratios\": [0.25, 1], \"hspace\": 0.05}\n",
        "    )\n",
        "\n",
        "    # Boxplot (small on top)\n",
        "    sns.boxplot(data=df_cat, x=col, y=target_var, orient=\"h\", ax=ax_box, palette=\"Set2\")\n",
        "    ax_box.set(xlabel=\"\")  \n",
        "    ax_box.set_ylabel(\"\")  \n",
        "    ax_box.set_title(f\"{col} distribution by {target_var}\", fontsize=12)\n",
        "\n",
        "    # Histogram (main plot)\n",
        "    sns.histplot(data=df_cat, x=col, hue=target_var, kde=True, element=\"step\", ax=ax_hist, palette=\"Set2\")\n",
        "    ax_hist.set_xlabel(col)\n",
        "    ax_hist.set_ylabel(\"Count\")\n",
        "    ax_hist.set_title(\"\")\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Borrowers who default tend to show the following general trends:\n",
        "    * Younger age\n",
        "    * Higher interest rates\n",
        "    * Lower income\n",
        "    * Shorter employment duration\n",
        "    * Larger loan amounts\n",
        "    * Slightly lower Credit Scores\n",
        "* This doesn't mean all defaulting customers have all these patterns at the same time, we just noticed a general trend\n",
        "* The `CreditScore` shows only a weak relationship with `Default`, indicating that its influence on predicting default risk is relatively minor compared to the other variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Conclusions and Next Steps\n",
        "\n",
        "The univariate and correlation analyses provided an initial understanding of the data distribution and key variable relationships with the target.\n",
        "\n",
        "In the univariate analysis we could observe that `NumCreditLines` and `LoanTerm` need to be treated as categorical variables. Additionally, we saw that all variables except the target are well-balanced and suitable for further modeling without extensive preprocessing. The target imbalance in `Default` should be addressed later during model development.\n",
        "\n",
        "The correlations between the features and the target variable are low, which indicates that multicollinearity is not a major concern, but also that no single variable strongly explains loan default.\n",
        "\n",
        "The analysis nevertheless showed that borrowers who default tend to show the following general trends:\n",
        "    * Younger age\n",
        "    * Higher interest rates\n",
        "    * Lower income\n",
        "    * Shorter employment duration\n",
        "    * Larger loan amounts\n",
        "    * Slightly lower Credit Scores\n",
        "\n",
        "Next Steps:\n",
        "* Data cleaning process: Remove irrelevant columns (e.g., LoanID) and encode variables correctly\n",
        "* Prepare data for feature engineering and modeling: scaling numerical variables, encoding categorical features, and creating new derived features if needed."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
