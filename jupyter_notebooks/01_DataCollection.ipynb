{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Data Collection Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objectives\n",
        "\n",
        "* Fetch data from Kaggle and save it as raw data.\n",
        "* Inspect the data and save it under outputs/datasets/collection\n",
        "\n",
        "## Inputs\n",
        "\n",
        "*   Kaggle JSON file - the authentication token.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Generate Dataset: outputs/datasets/collection/LoanDefaultDataset.csv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder, where the notebook is stored, to its parent folder\n",
        "* First we access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "* Then we want to make the parent of the current directory the new current directory\n",
        "    * os.path.dirname() gets the parent directory\n",
        "    * os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "current_dir = os.getcwd()\n",
        "print(f\"You set a new current directory: {current_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fetch data from Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install Kaggle package to fetch data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%pip install kaggle==1.7.4.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to authenticate Kaggle to download data in this session, your **authentication token (JSON file)** from Kaggle needs to be stored in the main project repository.\n",
        "* In case you don't have your token yet, please refer to the [Kaggle Documentation](https://www.kaggle.com/docs/api)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once you dropped your `kaggle.json` file in the main working directory, run the cell below, so the token is recognized in the session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
        "#! chmod 600 kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This project uses the [Loan Default Prediction Dataset](https://www.kaggle.com/datasets/nikhil1e9/loan-default)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the Kaggle dataset, and destination folder and download it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "KaggleDatasetPath = \"nikhil1e9/loan-default\"\n",
        "DestinationFolder = \"inputs/datasets/raw\"   \n",
        "! kaggle datasets download -d {KaggleDatasetPath} -p {DestinationFolder}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unzip the downloaded file, delete the zip file and delete the kaggle.json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import glob\n",
        "\n",
        "# Find all zip files in the folder\n",
        "zip_files = glob.glob(os.path.join(DestinationFolder, \"*.zip\"))\n",
        "\n",
        "# Extract each zip file and then delete it\n",
        "for zip_path in zip_files:\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(DestinationFolder)\n",
        "    os.remove(zip_path)  # remove the zip after extracting\n",
        "\n",
        "# Optionally, remove kaggle.json if it exists\n",
        "kaggle_json = os.path.join(os.getcwd(), \"kaggle.json\")\n",
        "if os.path.exists(kaggle_json):\n",
        "    os.remove(kaggle_json)\n",
        "\n",
        "print(\"All ZIP files extracted and deleted.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load and Inspect Kaggle data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(f\"inputs/datasets/raw/Loan_default.csv\")\n",
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The Dataset contains 255347 rows and 18 columns. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The dataset includes 10 numerical variables and 8 categorical ones. All data types are assigned appropriately:\n",
        "\n",
        "    - **Numerical variables** (e.g., `Age`, `Income`, `LoanAmount`, `CreditScore`, `DTIRatio`, etc.) are stored as either `int64` or `float64`.  \n",
        "    - **Categorical variables** (e.g., `Education`, `EmploymentType`, `MaritalStatus`, `LoanPurpose`, etc.) are stored as `object` type.  \n",
        "    - **Target variable** `Default` is also categorical (`int64`), representing 0 = non-default and 1 = default.\n",
        "\n",
        "    This indicates that the dataset is **properly typed** and no immediate conversions are required before preprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Number of NA values in each column:\")\n",
        "print(df.isna().sum())\n",
        "\n",
        "print(\"\\nTotal number of NA values in the dataframe:\", df.isna().sum().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* There are no missing values in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.describe().round(2).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.describe(include='object').T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Using `df.describe()`, summary statistics were generated for all variables.\n",
        "\n",
        "    - For **numerical features**, metrics such as `mean`, `std`, `min`, and `max` were reviewed to identify potential outliers or inconsistencies.\n",
        "    - For **categorical features** (via `df.describe(include='object')`), counts and most frequent categories were inspected to understand variable diversity and dominant groups.\n",
        "\n",
        "    Overall, this provides a comprehensive first look at both numerical and categorical distributions in the dataset.\n",
        "    The summary statistics indicate that the numerical variables are within reasonable ranges and there are no obvious anomalies (e.g., negative ages or zero income values).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select all object columns\n",
        "cat_cols = df.select_dtypes(include='object').columns\n",
        "\n",
        "# Print unique values per column\n",
        "for col in cat_cols:\n",
        "    print(f\"{col}: {df[col].unique()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Duplicated Entries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[df.duplicated(subset=['LoanID'])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* To ensure data integrity, the `LoanID` variable was checked for duplicate entries.  \n",
        "    A total of **0 duplicate IDs** were found, confirming that each loan record is unique.  \n",
        "\n",
        "* As the variable `LoanID` is a unique identifier for each record, it does not contribute to the prediction and will therefore be **excluded during the data cleaning step** before model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Target Variable Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The target variable **`Default`** indicates whether a borrower has defaulted on their loan (`1`) or not (`0`).  \n",
        "\n",
        "The class distribution is examined to understand the balance between default and non-default cases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Distribution of Loan Defaults:\")\n",
        "\n",
        "pd.DataFrame({\n",
        "    'Count': df['Default'].value_counts(),\n",
        "    'Percentage (%)': round(df['Default'].value_counts(normalize=True) * 100, 2)\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.countplot(x='Default', data=df, hue= \"Default\", palette='pastel')\n",
        "plt.title('Target Variable Distribution: Default')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(labels=['0 = No', '1 = Yes']) \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "- The target variable shows a **highly imbalanced** distribution.  \n",
        "- This is important because **imbalanced target classes** can bias models toward the majority class. We will have to perform oversampling in order to increase the representation of the minority class before training a model.\n",
        "\n",
        "At this stage, no transformation is applied yet, as the goal is to understand the target before data cleaning and modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "file_path = f'outputs/datasets/collection'\n",
        "variable_to_save = df\n",
        "filename = \"LoanDefaultData.csv\"\n",
        "\n",
        "# Try to generate output folder\n",
        "try:\n",
        "    os.makedirs(name=file_path)\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "# Save the dataset as csv file for further use\n",
        "variable_to_save.to_csv(f\"{file_path}/{filename}\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Conclusions and Next Steps\n",
        "\n",
        "The dataset appears complete and well-structured. Numerical variables are correctly typed, and categorical variables are stored as objects. Summary statistics show reasonable distributions, and the target variable Default is slightly imbalanced but usable. The LoanID column is unique for each record and does not contribute predictive value, so it will be removed in the data cleaning step. No missing or duplicate values were detected in the dataset.\n",
        "\n",
        "Next Steps:\n",
        "* Begin the data cleaning process: remove irrelevant columns (e.g., LoanID), handle any missing or inconsistent values if found, and encode categorical variables.\n",
        "* Conduct exploratory data analysis (EDA): visualize relationships between features and the target variable.\n",
        "* Prepare data for feature engineering and modeling: scaling numerical variables, encoding categorical features, and creating new derived features if needed."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
