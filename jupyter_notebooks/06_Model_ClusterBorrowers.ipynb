{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aStgWSO0E0E"
   },
   "source": [
    "# Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eLEkw5O0ECa"
   },
   "source": [
    "## Objectives\n",
    "\n",
    "* Fit and evaluate a cluster model to group similar data\n",
    "* Understand the profile for each cluster\n",
    "\n",
    "\n",
    "## Inputs\n",
    "* outputs/datasets/collection/LoanDefaultData.csv\n",
    "* Instructions on which variables to use for data cleaning and feature engineering. They are found in their respective notebooks.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* Cluster Pipeline\n",
    "* Train Set\n",
    "* Most important features to define a cluster plot\n",
    "* Clusters Profile Description\n",
    "* Cluster Silhouette\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is to: fit a Cluster ML pipeline\n",
    "that groups similar borrowers, and understands each cluster profile.\n",
    "We will also analyze how Default levels are distributed across the Clusters.\n",
    "\n",
    "It should be noted that we are now working with unsupervised learning\n",
    "as opposed to previously when we were working with supervised learning.\n",
    "Unsupervised Learning is slightly different due to one aspect:\n",
    "there is no target variable. The algorithm is left on its own to find patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uWZXH9LwoQg"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System & OS\n",
    "import os\n",
    "import warnings  # for ignoring warnings\n",
    "\n",
    "# Ignore FutureWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "# Inline plotting for VS Code / Jupyter\n",
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")  # consistent style\n",
    "import plotly.express as px  # interactive plots\n",
    "\n",
    "# Machine Learning Utilities\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler  # feature scaling\n",
    "from sklearn.feature_selection import SelectFromModel  # feature selection\n",
    "\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "\n",
    "# Classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Performance metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Feature Engineering (Feature-Engine)\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from feature_engine.selection import SmartCorrelatedSelection\n",
    "from feature_engine import transformation as vt  # log/power transformations\n",
    "\n",
    "# Joblib for saving/loading models\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to change the working directory from its current folder, where the notebook is stored, to its parent folder\n",
    "* First we access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then we want to make the parent of the current directory the new current directory\n",
    "    * os.path.dirname() gets the parent directory\n",
    "    * os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "current_dir = os.getcwd()\n",
    "print(f\"You set a new current directory: {current_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXKlJFX0iuM5"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mavJ8DibrcQ"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_var = \"loan_status\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First drop duplicates and then also the target variable `loan_status`, since we will analyze how Default levels are distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xk7DU_ekbtX8"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"outputs/datasets/collection/LoanDefaultData.csv\")\n",
    "df = df.drop_duplicates().drop(target_var, axis=1)\n",
    "\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krjAk78Tbyhv"
   },
   "source": [
    "# Cluster Pipeline with all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZWZHhpYaDjf"
   },
   "source": [
    "##  ML Cluster Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the initial clustering pipeline, we reuse the data cleaning and feature engineering steps developed in the feature engineering notebook to ensure consistency across all transformations.\n",
    "\n",
    "To simplify the preprocessing stage, all categorical variables are encoded using an arbitrary ordinal encoder, instead of applying different encoders per variable type. This keeps the pipeline lightweight and suitable for experimentation, while maintaining consistent numerical representation across features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6keis6ao8LA"
   },
   "outputs": [],
   "source": [
    "\n",
    "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "def PipelineCluster():\n",
    "    pipeline_base = Pipeline([\n",
    "        ('MedianImputer', MeanMedianImputer(\n",
    "            imputation_method='median',\n",
    "            variables=['person_emp_length', 'loan_int_rate']\n",
    "        )),\n",
    "        ('Winsorizer_iqr', Winsorizer(\n",
    "            capping_method='iqr',\n",
    "            fold=3,\n",
    "            tail='right',\n",
    "            variables=numeric_cols\n",
    "        )),\n",
    "        ('log_transform', vt.LogTransformer(\n",
    "            variables=['person_age', 'person_income', \n",
    "                    'cb_person_cred_hist_length']\n",
    "        )),\n",
    "        ('power_transform', vt.PowerTransformer(\n",
    "            variables=['person_emp_length', 'loan_amnt', \n",
    "                    'loan_percent_income']\n",
    "        )),\n",
    "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(\n",
    "            encoding_method='arbitrary',\n",
    "            variables=categorical_cols\n",
    "        )),\n",
    "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(\n",
    "            variables=None,\n",
    "            method=\"spearman\",\n",
    "            threshold=0.6,\n",
    "            selection_method=\"variance\"\n",
    "        )),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        # We use PCA to reduce dimensions and focus on relevant info\n",
    "        (\"PCA\", PCA(n_components=50, random_state=0)),\n",
    "        # KMeans is a good starting point for cluster analysis\n",
    "        (\"model\", KMeans(n_clusters=50, random_state=0)),\n",
    "    ])\n",
    "\n",
    "    return pipeline_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mrr31sD9DyvY"
   },
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finalize the cluster pipeline, we need to determine the most suitable number of principal components.\n",
    "Choosing the right ``n_components`` helps retain enough variance from the original data while removing noise and redundancy.\n",
    "\n",
    "We’ll analyze the explained variance ratio to find the smallest number of components that capture most of the dataset’s variability, and then update our pipeline accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can determine the optimal number of PCA components, we need to transform our dataset through all the preprocessing steps up to (but not including) the PCA and clustering stages.\n",
    "\n",
    "This ensures that all numerical features are properly scaled and categorical variables are encoded — as PCA requires numeric and standardized inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "es49S65qqvRw"
   },
   "outputs": [],
   "source": [
    "pipeline_cluster = PipelineCluster()\n",
    "pipeline_pca = Pipeline(pipeline_cluster.steps[:-2])\n",
    "df_pca = pipeline_pca.fit_transform(df)\n",
    "\n",
    "print(df_pca.shape,'\\n', type(df_pca))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After applying these steps, we obtain a clean and transformed feature matrix of shape (32416, 9), which will serve as input for our PCA analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlABEj9Iw6Jr"
   },
   "source": [
    "Next, we analyze how much variance in the dataset is explained by each principal component.\n",
    "By testing all 9 components (equal to the number of input features), we can visualize the explained variance ratio and identify the optimal number of components that retain most of the data’s information while reducing dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cM_Xsqxsrt5M"
   },
   "outputs": [],
   "source": [
    "n_components = 9 # initially set to number of columns\n",
    "\n",
    "def pca_components_analysis(df_pca, n_components):\n",
    "    pca = PCA(n_components=n_components).fit(df_pca)\n",
    "    x_PCA = pca.transform(df_pca)  # array with transformed PCA\n",
    "\n",
    "    ComponentsList = [\"Component \" + str(number)\n",
    "                      for number in range(n_components)]\n",
    "    dfExplVarRatio = pd.DataFrame(\n",
    "        data=np.round(100 * pca.explained_variance_ratio_, 3),\n",
    "        index=ComponentsList,\n",
    "        columns=['Explained Variance Ratio (%)'])\n",
    "\n",
    "    dfExplVarRatio['Accumulated Variance'] = (\n",
    "        dfExplVarRatio['Explained Variance Ratio (%)'].cumsum()\n",
    "    )\n",
    "\n",
    "    PercentageOfDataExplained = (dfExplVarRatio['Explained Variance Ratio (%)']\n",
    "                                 .sum())\n",
    "\n",
    "    print(\n",
    "        f\"* The {n_components} components explain \"\n",
    "        f\"{round(PercentageOfDataExplained, 2)}% of the data \\n\"\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    sns.lineplot(data=dfExplVarRatio,  marker=\"o\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(np.arange(0, 110, 10))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "pca_components_analysis(df_pca=df_pca, n_components=n_components)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the PCA explained variance plot, we observe that the first five components together explain approximately 72% of the total variance in the dataset. While adding more components would capture additional variance, the improvement beyond this point becomes marginal. \n",
    "\n",
    "Therefore, selecting 5 components provides a good balance between dimensionality reduction and information retention. This choice simplifies the feature space, reduces noise and redundancy, and keeps the clustering model more efficient and interpretable, while still preserving the majority of the data’s underlying structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_components_analysis(df_pca=df_pca,n_components=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update n_components to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PipelineCluster():\n",
    "    pipeline_base = Pipeline([\n",
    "        ('MedianImputer', MeanMedianImputer(\n",
    "            imputation_method='median',\n",
    "            variables=['person_emp_length', 'loan_int_rate']\n",
    "        )),\n",
    "        ('Winsorizer_iqr', Winsorizer(\n",
    "            capping_method='iqr',\n",
    "            fold=3,\n",
    "            tail='right',\n",
    "            variables=numeric_cols\n",
    "        )),\n",
    "        ('log_transform', vt.LogTransformer(\n",
    "            variables=['person_age', 'person_income', \n",
    "                    'cb_person_cred_hist_length']\n",
    "        )),\n",
    "        ('power_transform', vt.PowerTransformer(\n",
    "            variables=['person_emp_length', 'loan_amnt', \n",
    "                    'loan_percent_income']\n",
    "        )),\n",
    "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(\n",
    "            encoding_method='arbitrary',\n",
    "            variables=categorical_cols\n",
    "        )),\n",
    "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(\n",
    "            variables=None,\n",
    "            method=\"spearman\",\n",
    "            threshold=0.6,\n",
    "            selection_method=\"variance\"\n",
    "        )),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "\n",
    "        # we update n_components to 5\n",
    "        (\"PCA\", PCA(n_components=5, random_state=0)),\n",
    "\n",
    "        (\"model\", KMeans(n_clusters=50, random_state=0)),\n",
    "    ])\n",
    "    return pipeline_base\n",
    "\n",
    "\n",
    "PipelineCluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uw9NtDj4EtEJ"
   },
   "source": [
    "## Elbow Method and Silhouette Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we determine the optimal number of clusters using:\n",
    "\n",
    "1. Elbow Method – looks for the point where adding more clusters yields little improvement in compactness.\n",
    "2. Silhouette Score – measures how well-separated and cohesive the clusters are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we transform the dataset up to the PCA step so that it’s scaled and dimensionally reduced, ready for cluster evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVaMnb9vGyBw"
   },
   "outputs": [],
   "source": [
    "pipeline_cluster = PipelineCluster()\n",
    "pipeline_analysis = Pipeline(pipeline_cluster.steps[:-1]) # drop the KMeans step\n",
    "df_analysis = pipeline_analysis.fit_transform(df)\n",
    "\n",
    "print(df_analysis.shape,'\\n', type(df_analysis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the elbow method to identify a suitable number of clusters for our KMeans model. This technique provides a first estimate for the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZBcHjt7EwFT"
   },
   "outputs": [],
   "source": [
    "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(1,11)) \n",
    "visualizer.fit(df_analysis) \n",
    "visualizer.show() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The elbow analysis suggests k=4 as a suitable number of clusters. \n",
    "\n",
    "To further validate the choice, we perform silhouette analysis for a range of cluster numbers around the elbow point (k=2–5) to inspect cluster cohesion and separation. We do not consider k=1 because a single cluster would group all data together, providing no meaningful segmentation. \n",
    "\n",
    "Once confirmed, we will update the Cluster Pipeline with the chosen number of clusters and refit it to assign cluster labels for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6 is not inclusive, it will stop at 5\n",
    "n_cluster_start, n_cluster_stop = 2, 6\n",
    "\n",
    "print(\"=== Average Silhouette Score for different number of clusters ===\")\n",
    "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(\n",
    "    n_cluster_start, n_cluster_stop), metric='silhouette')\n",
    "visualizer.fit(df_analysis)\n",
    "visualizer.show()\n",
    "plt.show()\n",
    "print(\"\\n\")\n",
    "\n",
    "for n_clusters in np.arange(start=n_cluster_start, stop=n_cluster_stop):\n",
    "\n",
    "    print(f\"=== Silhouette plot for {n_clusters} Clusters ===\")\n",
    "    visualizer = SilhouetteVisualizer(estimator=KMeans(n_clusters=n_clusters, \n",
    "                                                       random_state=0),\n",
    "                                      colors='yellowbrick')\n",
    "    visualizer.fit(df_analysis)\n",
    "    visualizer.show()\n",
    "    plt.show()\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Silhouette Elbow Plot suggests k = 2.\n",
    "* Therefore we examine the silhouette plots for 2, 3, and 4 clusters (because the elbow method suggested 4 clusters).\n",
    "* None of the clusters in these plots have silhouette scores below the average.\n",
    "* The average silhouette score is highest for 3 clusters\n",
    "* As a result, we select k = 3 as the optimal number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update n_clusters to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PipelineCluster():\n",
    "    pipeline_base = Pipeline([\n",
    "        ('MedianImputer', MeanMedianImputer(\n",
    "            imputation_method='median',\n",
    "            variables=['person_emp_length', 'loan_int_rate']\n",
    "        )),\n",
    "        ('Winsorizer_iqr', Winsorizer(\n",
    "            capping_method='iqr',\n",
    "            fold=3,\n",
    "            tail='right',\n",
    "            variables=numeric_cols\n",
    "        )),\n",
    "        ('log_transform', vt.LogTransformer(\n",
    "            variables=['person_age', 'person_income', \n",
    "                    'cb_person_cred_hist_length']\n",
    "        )),\n",
    "        ('power_transform', vt.PowerTransformer(\n",
    "            variables=['person_emp_length', 'loan_amnt', \n",
    "                    'loan_percent_income']\n",
    "        )),\n",
    "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(\n",
    "            encoding_method='arbitrary',\n",
    "            variables=categorical_cols\n",
    "        )),\n",
    "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(\n",
    "            variables=None,\n",
    "            method=\"spearman\",\n",
    "            threshold=0.6,\n",
    "            selection_method=\"variance\"\n",
    "        )),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "\n",
    "        (\"PCA\", PCA(n_components=5, random_state=0)),\n",
    "\n",
    "        # we update n_clusters to 3\n",
    "        (\"model\", KMeans(n_clusters=3, random_state=0)),\n",
    "    ])\n",
    "    return pipeline_base\n",
    "\n",
    "\n",
    "PipelineCluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQBjAlRsHhU4"
   },
   "source": [
    "## Fit Cluster Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpxaylKk-6CQ"
   },
   "source": [
    "Now that we have determined the optimal number of clusters (k = 3) and selected the number of PCA components, we fit the full cluster pipeline to the dataset. This transforms the data, applies scaling, PCA, and generates cluster assignments for each observation, which we can later analyze and profile.\n",
    "\n",
    "We use the full dataset for clustering since there is no target variable to create a separate test set and evaluate performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zfKHc63v-6Zm"
   },
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "print(X.shape)\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfRpKC4Ykreg"
   },
   "source": [
    "Initialize the clustering pipeline and fit it to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MAiyUpTWHjQh"
   },
   "outputs": [],
   "source": [
    "pipeline_cluster = PipelineCluster()\n",
    "pipeline_cluster.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0L0iMkjJHXSI"
   },
   "source": [
    "## Add cluster predictions to dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKT5IjmTmei8"
   },
   "source": [
    "Assign cluster labels from the fitted KMeans model to the original dataframe\n",
    "* ``pipeline_cluster['model'].labels_`` contains the cluster assignment for each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ow8B0xVdmlgK"
   },
   "outputs": [],
   "source": [
    "X['Clusters'] = pipeline_cluster['model'].labels_\n",
    "print(X.shape)\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we display the relative frequency of each cluster to understand how the data is distributed across clusters. This gives a sense of which clusters are larger or smaller. Addittionally, we visualize the absolute number of records in each cluster using a bar chart, making it easier to compare cluster sizes at a glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAVrYJEqxYyG"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"* Clusters frequencies \\n\"\n",
    "    f\"{X['Clusters'].value_counts(normalize=True).to_frame().round(2)}\\n\\n\"\n",
    ")\n",
    "\n",
    "X['Clusters'].value_counts().sort_values().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The analysis shows that the dataset is relatively balanced across the three clusters.\n",
    "Cluster 1 and Cluster 2 each contain roughly 40% of the records, while Cluster 0 is smaller, representing around 20% of the dataset.\n",
    "* This distribution still allows for meaningful differentiation between customer segments, while highlighting that Cluster 0 represents a more distinct, less common borrower profile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we visualize the PCA components and the clusters.\n",
    "\n",
    "To simplify, we create a 2D scatter plot using the first two PCA components, coloring each point according to its assigned cluster. Additionally, we plot the cluster centroids, which represent the center of each cluster as calculated by the KMeans algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=df_analysis[:, 0], y=df_analysis[:, 1],\n",
    "                hue=X['Clusters'], palette='Set1', alpha=0.6)\n",
    "plt.scatter(x=pipeline_cluster['model'].cluster_centers_[:, 0], \n",
    "            y=pipeline_cluster['model'].cluster_centers_[:, 1],\n",
    "            marker=\"x\", s=169, linewidths=3, color=\"black\")\n",
    "plt.xlabel(\"PCA Component 0\")\n",
    "plt.ylabel(\"PCA Component 1\")\n",
    "plt.title(\"PCA Components colored by Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each point represents a borrower and is colored according to its cluster. The PCA components combine multiple original features, so the axes themselves are abstract and don’t have a direct business interpretation.\n",
    "\n",
    "* In the plot, we can see three distinct clusters, with most points clearly separated. There is some overlap along the cluster borders, but this seems minimal, indicating that the clustering has generally captured meaningful differences between the groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnjHhYjXng2r"
   },
   "source": [
    "We save the cluster predictions from this pipeline to use in the future. We will get back to that in a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWgb0kPOWtMa"
   },
   "outputs": [],
   "source": [
    "cluster_predictions_with_all_variables = X['Clusters']\n",
    "cluster_predictions_with_all_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTWTf1rgkQ7b"
   },
   "source": [
    "## Fit a classifier, where the target is cluster predictions and features remaining variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we have assigned each borrower to a cluster, but we have not yet interpreted what each cluster represents.\n",
    "\n",
    "Next, we will fit a classifier pipeline using the cluster labels as the target variable and the remaining features as inputs. The goal of this approach is to identify which features are most important in defining the clusters. By examining the classifier’s feature importances, we can gain insight into the characteristics that distinguish each group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hP6sGUn0XyDm"
   },
   "source": [
    "We copy `X` to a DataFrame `df_clf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OeLq81sm2yAg"
   },
   "outputs": [],
   "source": [
    "df_clf = X.copy()\n",
    "print(df_clf.shape)\n",
    "df_clf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4b3Ei6Os5X3s"
   },
   "source": [
    "Split into Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cgHXehCVyzUl"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_clf.drop(['Clusters'], axis=1),\n",
    "    df_clf['Clusters'],\n",
    "    test_size=0.2,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EZUk-uV5aN8"
   },
   "source": [
    "Create classifier pipeline steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PipelineClf2ExplainClusters():\n",
    "    pipeline_base = Pipeline([\n",
    "        ('MedianImputer', MeanMedianImputer(\n",
    "            imputation_method='median',\n",
    "            variables=['person_emp_length', 'loan_int_rate']\n",
    "        )),\n",
    "        ('Winsorizer_iqr', Winsorizer(\n",
    "            capping_method='iqr',\n",
    "            fold=5,\n",
    "            tail='right',\n",
    "            variables=numeric_cols\n",
    "        )),\n",
    "        ('log_transform', vt.LogTransformer(\n",
    "            variables=['person_age', 'person_income', \n",
    "                    'cb_person_cred_hist_length']\n",
    "        )),\n",
    "        ('power_transform', vt.PowerTransformer(\n",
    "            variables=['person_emp_length', 'loan_amnt', \n",
    "                    'loan_percent_income']\n",
    "        )),\n",
    "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(\n",
    "            encoding_method='arbitrary',\n",
    "            variables=categorical_cols\n",
    "        )),\n",
    "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(\n",
    "            variables=None,\n",
    "            method=\"spearman\",\n",
    "            threshold=0.6,\n",
    "            selection_method=\"variance\"\n",
    "        )),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "\n",
    "        (\"feat_selection\", SelectFromModel(\n",
    "            GradientBoostingClassifier(random_state=0))),\n",
    "\n",
    "        # use an algorithm that typically offers good results, and feature importance\n",
    "        # can be assessed with .features_importance_\n",
    "        (\"model\", GradientBoostingClassifier(random_state=0)),\n",
    "\n",
    "    ])\n",
    "    return pipeline_base\n",
    "\n",
    "\n",
    "PipelineClf2ExplainClusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the classifier to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3R7xdg1Av0Ce"
   },
   "outputs": [],
   "source": [
    "pipeline_clf_cluster = PipelineClf2ExplainClusters()\n",
    "pipeline_clf_cluster.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z05LMFoZ4T2K"
   },
   "source": [
    "## Evaluate classifier performance on Train and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our clusters are generally well balanced, so there is no target imbalance to worry about.\n",
    "\n",
    "If the clusters had been imbalanced, we might have needed to treat the target imbalance before fitting the classifier. In this case, however, we can proceed to fit the model without any special handling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage of the workflow, we could also conduct a detailed hyperparameter\n",
    "optimization to find the best model. However, we are only interested in finding a pipeline\n",
    "that can explain the relationship between the target - Clusters - and the features to assess\n",
    "the feature importance afterwards. As a result, we will omit this step from the workflow here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M1iqL2Kc544K"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, pipeline_clf_cluster.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Oo4xJMZ615p"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pipeline_clf_cluster.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier performs very well in the train and test set in predicting the cluster labels, achieving an overall accuracy of 85%.\n",
    "\n",
    "All three clusters are predicted with balanced precision and recall scores, indicating that the model effectively captures the defining characteristics of each segment.\n",
    "Cluster 0 shows slightly lower recall, suggesting that it is somewhat harder to distinguish compared to the other two clusters, but overall, the classifier demonstrates consistent and reliable performance.\n",
    "This confirms that the selected features are informative for differentiating between the three borrower profiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEwjHBSh5ejG"
   },
   "source": [
    "## Assess the most important Features that define a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After clustering and fitting the classifier pipeline, we want to identify which features are most important in defining the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BG5ztHxsKcd5"
   },
   "outputs": [],
   "source": [
    "# Extract the feature names after the preprocessing steps but before \n",
    "# feature selection\n",
    "# Number of preprocessing steps in the pipeline that change the feature space\n",
    "data_cleaning_feat_eng_steps = 6 \n",
    "columns_after_data_cleaning_feat_eng = (\n",
    "    Pipeline(pipeline_clf_cluster.steps[:data_cleaning_feat_eng_steps])\n",
    "    .transform(X_train)\n",
    "    .columns\n",
    ")\n",
    "\n",
    "# Select only the features that were retained by the pipeline's feature \n",
    "# selection step\n",
    "best_features = columns_after_data_cleaning_feat_eng[\n",
    "    pipeline_clf_cluster['feat_selection'].get_support()\n",
    "].to_list()\n",
    "\n",
    "# Create a DataFrame showing feature names and their importance according to \n",
    "# the trained model\n",
    "df_feature_importance = pd.DataFrame(\n",
    "    data={\n",
    "        'Feature': columns_after_data_cleaning_feat_eng[\n",
    "            pipeline_clf_cluster['feat_selection'].get_support()\n",
    "        ],\n",
    "        'Importance': pipeline_clf_cluster['model'].feature_importances_\n",
    "    }\n",
    ").sort_values(by='Importance', ascending=False)  \n",
    "\n",
    "# Update best_features list to reflect the sorted order by importance\n",
    "best_features = df_feature_importance['Feature'].to_list()\n",
    "\n",
    "# Print the most important features and how the model was trained on them\n",
    "print(\n",
    "    f\"* These are the {len(best_features)} most important features \"\n",
    "    f\"in descending order:\\n{best_features}\\n\"\n",
    ")\n",
    "\n",
    "# Plot the feature importance for visual interpretation\n",
    "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
    "plt.title('Feature Importance for Cluster Classifier')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgul0EF9nx_E"
   },
   "source": [
    "We will store the best_features to use at a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YzyMkwHznyG8"
   },
   "outputs": [],
   "source": [
    "best_features_pipeline_all_variables = best_features\n",
    "best_features_pipeline_all_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'cb_person_default_on_file', 'person_home_ownership' and 'person_income' are the variables that most help to define the clusters.\n",
    "We can now use these variables to explain each cluster's profile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2ywCxJmkRQn"
   },
   "source": [
    "## Cluster Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZMr-wiudEkb"
   },
   "source": [
    "Next, we will set up two custom functions to analyze and describe our clusters:\n",
    "\n",
    "* DescriptionAllClusters – creates a summary table for each cluster, showing the most common values for categorical variables and the interquartile range (IQR) for numerical variables. This will help us understand the key characteristics of each cluster.\n",
    "\n",
    "* cluster_distribution_per_variable – visualizes how clusters are distributed across a given variable (e.g., loan_status) and calculates the relative percentage of each target level within clusters. This helps us see patterns and differences between clusters with respect to important outcomes.\n",
    "\n",
    "We will define these functions now and use them afterwards to explore and interpret the cluster profiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. DescriptionAllClusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lpRVDqTdEul"
   },
   "outputs": [],
   "source": [
    "\n",
    "def DescriptionAllClusters(df, decimal_points=3):\n",
    "\n",
    "    DescriptionAllClusters = pd.DataFrame(\n",
    "        columns=df.drop(['Clusters'], axis=1).columns)\n",
    "    # iterate on each cluster , calls Clusters_IndividualDescription()\n",
    "    for cluster in df.sort_values(by='Clusters')['Clusters'].unique():\n",
    "\n",
    "        EDA_ClusterSubset = df.query(\n",
    "            f\"Clusters == {cluster}\").drop(['Clusters'], axis=1)\n",
    "        ClusterDescription = Clusters_IndividualDescription(\n",
    "            EDA_ClusterSubset, cluster, decimal_points)\n",
    "        DescriptionAllClusters = pd.concat(\n",
    "            [ClusterDescription, DescriptionAllClusters])\n",
    "\n",
    "    DescriptionAllClusters.set_index(['Cluster'], inplace=True)\n",
    "    return DescriptionAllClusters\n",
    "\n",
    "\n",
    "def Clusters_IndividualDescription(EDA_Cluster, cluster, decimal_points):\n",
    "\n",
    "    ClustersDescription = pd.DataFrame(columns=EDA_Cluster.columns)\n",
    "    # for a given cluster, iterate over all columns\n",
    "    # if the variable is numerical, calculate the IQR: display as Q1 -- Q3.\n",
    "    # That will show the range for the most common values for the numerical \n",
    "    # variable\n",
    "    # if the variable is categorical, count the frequencies and displays the \n",
    "    # top 3 most frequent\n",
    "    # That will show the most common levels for the category\n",
    "\n",
    "    for col in EDA_Cluster.columns:\n",
    "\n",
    "        try: \n",
    "\n",
    "            if EDA_Cluster[col].dtypes == 'object':\n",
    "\n",
    "                top_frequencies = (EDA_Cluster\n",
    "                                   .dropna(subset=[col])[[col]]\n",
    "                                   .value_counts(normalize=True)\n",
    "                                   .nlargest(n=3))\n",
    "                Description = ''\n",
    "\n",
    "                for x in range(len(top_frequencies)):\n",
    "                    freq = top_frequencies.iloc[x]\n",
    "                    category = top_frequencies.index[x][0]\n",
    "                    CategoryPercentage = int(round(freq*100, 0))\n",
    "                    statement = f\"'{category}': {CategoryPercentage}% , \"\n",
    "                    Description = Description + statement\n",
    "\n",
    "                ClustersDescription.at[0, col] = Description[:-2]\n",
    "\n",
    "            elif EDA_Cluster[col].dtypes in ['float64', 'int64']:\n",
    "                DescStats = EDA_Cluster.dropna(subset=[col])[[col]].describe()\n",
    "                Q1 = round(DescStats.loc['25%', col], decimal_points)\n",
    "                Q3 = round(DescStats.loc['75%', col], decimal_points)\n",
    "                Description = f\"{Q1} -- {Q3}\"\n",
    "                ClustersDescription.at[0, col] = Description\n",
    "\n",
    "        except Exception as e:\n",
    "            ClustersDescription.at[0, col] = 'Not available'\n",
    "            print(\n",
    "                f\"** Error Exception: {e} - cluster {cluster}, variable {col}\")\n",
    "\n",
    "    ClustersDescription['Cluster'] = str(cluster)\n",
    "\n",
    "    return ClustersDescription\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHo7wmH68AYc"
   },
   "source": [
    "2. cluster_distribution_per_variable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NN23X2dT8AeA"
   },
   "outputs": [],
   "source": [
    "def cluster_distribution_per_variable(df, target):\n",
    "    \"\"\"\n",
    "    The data should have 2 variables, the cluster predictions and\n",
    "    the variable you want to analyze with, in this case we call \"target\".\n",
    "    We use plotly express to create 2 plots:\n",
    "    Cluster distribution across the target.\n",
    "    Relative presence of the target level in each cluster.\n",
    "    \"\"\"\n",
    "    df_bar_plot = (df.groupby(['Clusters', target])\n",
    "                   .size().reset_index(name='Count'))\n",
    "    df_bar_plot.columns = ['Clusters', target, 'Count']\n",
    "    df_bar_plot[target] = df_bar_plot[target].astype('object')\n",
    "\n",
    "    print(f\"Clusters distribution across {target} levels\")\n",
    "    fig = px.bar(df_bar_plot, x='Clusters', y='Count',\n",
    "                 color=target, width=800, height=500)\n",
    "    fig.update_layout(xaxis=dict(tickmode='array',\n",
    "                      tickvals=df['Clusters'].unique()))\n",
    "    fig.show(renderer='jupyterlab')\n",
    "\n",
    "    df_relative = (df\n",
    "                   .groupby([\"Clusters\", target])\n",
    "                   .size()\n",
    "                   .unstack(fill_value=0)\n",
    "                   .apply(lambda x: 100 * x / x.sum(), axis=1)\n",
    "                   .stack()\n",
    "                   .reset_index(name='Relative Percentage (%)')\n",
    "                   .sort_values(by=['Clusters', target])\n",
    "                   )\n",
    "\n",
    "    print(f\"Relative Percentage (%) of {target} in each cluster\")\n",
    "    fig = px.line(df_relative, x='Clusters', y='Relative Percentage (%)',\n",
    "                  color=target, width=800, height=500)\n",
    "    fig.update_layout(xaxis=dict(tickmode='array',\n",
    "                      tickvals=df['Clusters'].unique()))\n",
    "    fig.update_traces(mode='markers+lines')\n",
    "    fig.show(renderer='jupyterlab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtD0Y3NdJOhm"
   },
   "source": [
    "### Cluster profile based on the best features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73J7J65v4O_d"
   },
   "source": [
    "Before using the ``DescriptionAllClusters`` function, we prepare the data as follows:\n",
    "\n",
    "1. Select relevant features – We create ``df_cluster_profile`` from our classifier dataset, keeping only the best features (as determined from feature importance) and the cluster labels. This ensures that the description focuses on the most relevant variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PztdhjGl4Vkg"
   },
   "outputs": [],
   "source": [
    "df_cluster_profile = df_clf.copy()\n",
    "df_cluster_profile = df_cluster_profile.filter(items=best_features + ['Clusters'], axis=1)\n",
    "print(df_cluster_profile.shape)\n",
    "df_cluster_profile.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mfJRrFc7wzu"
   },
   "source": [
    "2. Prepare the target variable – We load the default data (df_default), drop duplicates, and keep only the target variable. Since it is encoded as an integer but represents a categorical outcome, we convert it to type object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSRSNqiF4mnm"
   },
   "outputs": [],
   "source": [
    "df_default = pd.read_csv(\"outputs/datasets/collection/LoanDefaultData.csv\")\n",
    "# Default is encoded as an integer,\n",
    "# even being a categorical variable. Therefore for this analysis we'll \n",
    "# change its data type to 'object'.\n",
    "# for cluster analysis plot (!)\n",
    "df_default = df_default.drop_duplicates().filter([target_var])\n",
    "# df_default = df_default\n",
    "print(df_default.shape)\n",
    "df_default[target_var] = df_default[target_var].astype('object')\n",
    "df_default.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Combine features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([\n",
    "    df_cluster_profile.reset_index(drop=True),\n",
    "    df_default.reset_index(drop=True)\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate cluster description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LDhycaSEdORm"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "clusters_profile = DescriptionAllClusters(df=df_combined, decimal_points=0)\n",
    "clusters_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster Description using the most important features:\n",
    "\n",
    "* Cluster 0: Borrowers with a history of previous defaults: mostly renters, with moderate incomes and the highest default rate (41%). This group represents a higher-risk segment.\n",
    "* Cluster 1: Borrowers who primarily have mortgages, earn higher incomes, and rarely default (only 9%). This is the most financially stable and lowest-risk group.\n",
    "* Cluster 2: Borrowers who mostly rent, have lower to mid-range incomes, and moderate default rates (24%). They generally have no prior default record but represent a middle-risk segment between Clusters 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SS6CCCb74lH"
   },
   "source": [
    "### Clusters distribution across Default levels & Relative Percentage of Default in each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to analyze how the clusters relate to loan defaults.\n",
    "We create a new DataFrame containing the predicted cluster for each user and their default status. Using cluster_distribution_per_variable, we will visualize:\n",
    "\n",
    "* The absolute distribution of clusters colored by default level.\n",
    "* The relative percentage of default levels within each cluster.\n",
    "\n",
    "This helps us understand which clusters are more prone to default and provides actionable insights for risk management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kwEUdPI2NHOb"
   },
   "outputs": [],
   "source": [
    "df_cluster_vs_Default=  df_default.copy()\n",
    "df_cluster_vs_Default['Clusters'] = X['Clusters']\n",
    "cluster_distribution_per_variable(df=df_cluster_vs_Default, target=target_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In summary, we can see that borrowers in Cluster 0 tend to default the most (41%), followed by Cluster 2 with a moderate default rate (24%). Cluster 1 borrowers are the most stable, with only 9% defaulting.\n",
    "\n",
    "**Business interpretation:**  \n",
    "To mitigate default risk, borrowers in Cluster 0 should receive closer monitoring, stricter underwriting, or smaller loan approvals. Cluster 2 borrowers may qualify for standard terms but could benefit from proactive financial counseling or tighter credit limits. Cluster 1 borrowers, being the most reliable, can be prioritized for favorable loan conditions or loyalty-based incentives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEfOvjx8ZhAH"
   },
   "source": [
    "# Fit New Cluster Pipeline with most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFno4XnSZlZV"
   },
   "source": [
    "To simplify the cluster pipeline and reduce the number of variables, we will compare the original cluster pipeline (using all features) with a new pipeline that uses only the variables identified as most important for defining the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ROH8cre2PHWx"
   },
   "outputs": [],
   "source": [
    "best_features_pipeline_all_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLOL2zr4Jr68"
   },
   "source": [
    "## Define trade-off and metrics to compare new and previous Cluster Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJyxkSowZ9Cm"
   },
   "source": [
    "The evaluation will follow these steps:\n",
    "\n",
    "1. Apply the elbow method and silhouette analysis to see if the same number of clusters is suggested.\n",
    "2. Fit the new cluster pipeline and compare its predictions to the previous pipeline to check for consistency.\n",
    "3. Fit a classifier to explain the clusters and compare Train/Test performance to the original pipeline.\n",
    "4. Verify if the classifier identifies the same most important features as before.\n",
    "5. Compare the cluster profiles from both pipelines to check if they are equivalent.\n",
    "\n",
    "If the new pipeline passes these checks, it can be used in production to predict clusters for prospects using fewer variables, which is particularly beneficial for real-time applications.\n",
    "\n",
    "If not all criteria are met, it becomes a contextual decision: stakeholders must decide if they accept the trade-offs and interpretations of the new pipeline. There is no strict right or wrong in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxyTpm4EJx4s"
   },
   "source": [
    "## Subset data with the most relevant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQu9033oZ6r9"
   },
   "outputs": [],
   "source": [
    "df_reduced = df.filter(best_features_pipeline_all_variables)\n",
    "df_reduced.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ub9_YoIeaS5"
   },
   "source": [
    "## Rewrite Cluster Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PrlQuieZeaS6"
   },
   "outputs": [],
   "source": [
    "def PipelineCluster():\n",
    "    pipeline_base = Pipeline([\n",
    "        ('Winsorizer_iqr', \n",
    "         Winsorizer(capping_method='iqr', fold=3, tail='right', \n",
    "                    variables=['person_income'])),\n",
    "        ('log_transform', \n",
    "         vt.LogTransformer(variables=['person_income'])),\n",
    "        (\"OrdinalCategoricalEncoder\", \n",
    "         OrdinalEncoder(encoding_method='arbitrary',\n",
    "                        variables=[\"person_home_ownership\", \n",
    "                                   \"cb_person_default_on_file\"])),\n",
    "        \n",
    "        # it doesn't need SmartCorrelation\n",
    "        \n",
    "        (\"scaler\", StandardScaler()),\n",
    "        \n",
    "        # No PCA step needed, since we know which features to consider\n",
    "\n",
    "        (\"model\", KMeans(n_clusters=3, random_state=0))\n",
    "\n",
    "    ])\n",
    "    return pipeline_base\n",
    "\n",
    "\n",
    "PipelineCluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D57ncdQ7hBXe"
   },
   "source": [
    "## Apply Elbow Method and Silhouette analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0wqQOM3hBXr"
   },
   "outputs": [],
   "source": [
    "pipeline_cluster = PipelineCluster()\n",
    "pipeline_analysis = Pipeline(pipeline_cluster.steps[:-1])\n",
    "df_analysis = pipeline_analysis.fit_transform(df_reduced)\n",
    "\n",
    "print(df_analysis.shape,'\\n', type(df_analysis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_1H05FKhBXs"
   },
   "source": [
    "Elbow Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsAJW4s0hBXt"
   },
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(1,11))\n",
    "visualizer.fit(df_analysis) \n",
    "visualizer.show() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We note that this plot suggests 4 clusters\n",
    "and that a sharp fall happens between 2 and 4, so we can pick this range for Silhouette analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_cluster_start, n_cluster_stop = 2, 5\n",
    "\n",
    "print(\"=== Average Silhouette Score for different number of clusters ===\")\n",
    "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(\n",
    "    n_cluster_start, n_cluster_stop), metric='silhouette')\n",
    "visualizer.fit(df_analysis)\n",
    "visualizer.show()\n",
    "plt.show()\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "for n_clusters in np.arange(start=n_cluster_start, stop=n_cluster_stop):\n",
    "\n",
    "    print(f\"=== Silhouette plot for {n_clusters} Clusters ===\")\n",
    "    visualizer = SilhouetteVisualizer(estimator=KMeans(n_clusters=n_clusters, \n",
    "                                                       random_state=0),\n",
    "                                      colors='yellowbrick')\n",
    "    visualizer.fit(df_analysis)\n",
    "    visualizer.show()\n",
    "    plt.show()\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Silhouette Elbow Plot suggests k = 3\n",
    "\n",
    "* The silhouette plots confirm this choice: the three-cluster solution shows the highest average silhouette score, no negative silhouette values, and both clusters exceed the average score. \n",
    "\n",
    "* Therefore, we select k = 3 as the optimal number of clusters for this dataset. The silhouette score of 0.52 exceeds our success metric of minimum 0.45."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_T1gtprhe8W"
   },
   "source": [
    "## Fit New Cluster Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEtzurhOhe8W"
   },
   "source": [
    "We set X as our training set for the cluster. It is a copy of df_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BIZRR3wChe8X"
   },
   "outputs": [],
   "source": [
    "X = df_reduced.copy()\n",
    "print(X.shape)\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-z3ST3nhe8Z"
   },
   "source": [
    "Fit Cluster pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAaEPy3Uhe8Z"
   },
   "outputs": [],
   "source": [
    "pipeline_cluster = PipelineCluster()\n",
    "pipeline_cluster.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Pdrt5bdKLDF"
   },
   "source": [
    "## Add cluster predictions to dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdFwZk1ihe8b"
   },
   "source": [
    "We add a column \"`Clusters`\" (with the cluster pipeline predictions) to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['Clusters'] = pipeline_cluster['model'].labels_\n",
    "print(X.shape)\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUwR8vCEhe8b"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"* Clusters frequencies \\n\"\n",
    "    f\"{X['Clusters'].value_counts(normalize=True).to_frame().round(2)} \\n\\n\"\n",
    ")\n",
    "X['Clusters'].value_counts().sort_values().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cluster distribution shows that Cluster 1 now represents the largest share with 45% of the data, followed by Cluster 2 with 38%, and Cluster 0 with 18%.\n",
    "Compared to the previous, more balanced distribution, the new model produces a slightly more uneven split, indicating that borrowers in Cluster 1 are somewhat more prevalent in the dataset.\n",
    "\n",
    "As the difference to the previous model is not too large, we consider it to be okay. But how similar are the predictions from this cluster\n",
    "and the previous cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUxjeMypKOUe"
   },
   "source": [
    "## Compare current cluster predictions to previous cluster predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6721kuGiII6"
   },
   "source": [
    "We just fitted a new cluster pipeline and want to compare if its predictions are \"equivalent\" to the previous cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAS2dDXQhe8c"
   },
   "source": [
    "These are the predictions from the **previous** cluster pipeline - trained with all variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbVaUAABhe8c"
   },
   "outputs": [],
   "source": [
    "cluster_predictions_with_all_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUiHzLIwimaD"
   },
   "source": [
    "And these are the predictions from the **current** cluster pipeline (trained with `['cb_person_default_on_file', 'person_home_ownership', 'person_income']`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following analysis steps, it becomes apparent that the cluster labels between the predictions using all variables and those using only the best features are inverted for cluster 1 and 2. To ensure both predictions are comparable, we reassign the labels in the reduced-feature predictions accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_kC7cKKCiwD5"
   },
   "outputs": [],
   "source": [
    "cluster_predictions_with_best_features = (\n",
    "    X['Clusters'].replace({1: 99, 2: 1}).replace({99: 2}))\n",
    "cluster_predictions_with_best_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Jt-2n1GidBa"
   },
   "source": [
    "We use a confusion matrix to evaluate if the predictions of both pipelines are **\"equivalent\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLy37N1TiiSx"
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(cluster_predictions_with_all_variables, \n",
    "                       cluster_predictions_with_best_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows a strong agreement between the two clustering pipelines (the original one and the reduced-feature one):\n",
    "\n",
    "* The majority of records are consistently assigned to the same cluster\n",
    "* Only a relatively small number of records are assigned differently across the two pipelines, indicating minor discrepancies\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "* The two pipelines produce very similar cluster assignments, meaning the reduced-feature pipeline (using only the most important variables) effectively captures the same structure in the data.\n",
    "This makes it a solid candidate for deployment, it’s simpler, faster, and nearly as accurate as the full version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcPfalkwmomc"
   },
   "source": [
    "## Fit a classifier, where the target is cluster predictions and features remaining variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are again in a position where we can fit a cluster pipeline but we will first need\n",
    "clarification on the most relevant features for the clusters. We will use a classifier\n",
    "pipeline to help in this task using the supervised learning workflow that we're already familiar with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_XoL6tuRmomf"
   },
   "outputs": [],
   "source": [
    "df_clf = X.copy()\n",
    "print(df_clf.shape)\n",
    "df_clf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSfwpL-Fmomf"
   },
   "source": [
    "Split into Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPyXs27Kmomf"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_clf.drop(['Clusters'], axis=1),\n",
    "    df_clf['Clusters'],\n",
    "    test_size=0.2,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0W-cV2ts8A_N"
   },
   "source": [
    "Rewrite pipeline to explain clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lm63GRYP8BIV"
   },
   "outputs": [],
   "source": [
    "def PipelineClf2ExplainClusters():\n",
    "    pipeline_base = Pipeline([\n",
    "        ('Winsorizer_iqr', Winsorizer(capping_method='iqr', \n",
    "                                      fold=3, tail='right', \n",
    "                                      variables=['person_income'])),\n",
    "        ('log_transform', vt.LogTransformer(variables=['person_income'])),\n",
    "        (\"OrdinalCategoricalEncoder\", \n",
    "         OrdinalEncoder(encoding_method='arbitrary',\n",
    "                        variables=[\"person_home_ownership\", \n",
    "                                   \"cb_person_default_on_file\"])),\n",
    "        \n",
    "        # it doesn't need SmartCorrelation\n",
    "\n",
    "        (\"scaler\", StandardScaler()),\n",
    "\n",
    "        # we don't consider feature selection step, since we know which features to consider\n",
    "\n",
    "        (\"model\", GradientBoostingClassifier(random_state=0)),\n",
    "\n",
    "    ])\n",
    "    return pipeline_base\n",
    "\n",
    "\n",
    "PipelineClf2ExplainClusters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkaBhgjOmomg"
   },
   "source": [
    "## Fit a classifier, where the target is cluster labels and features remaining variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zU6mwsFYmomg"
   },
   "source": [
    "Create and fit a classifier pipeline to learn the feature importance when defining a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3liI7qjmomg"
   },
   "outputs": [],
   "source": [
    "pipeline_clf_cluster = PipelineClf2ExplainClusters()\n",
    "pipeline_clf_cluster.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hCk6Swrmomh"
   },
   "source": [
    "## Evaluate classifier performance on Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjXpF0x8momh"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_train, pipeline_clf_cluster.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-Obn_Hcmomh"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pipeline_clf_cluster.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier shows good performance in both the train and the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G07251XWmomh"
   },
   "source": [
    "## Assess Most Important Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best features are now the columns of our train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMTUNBYN8fyf"
   },
   "outputs": [],
   "source": [
    "best_features = X_train.columns.to_list()\n",
    "\n",
    "# create a DataFrame to display feature importance\n",
    "df_feature_importance = (pd.DataFrame(data={\n",
    "    'Feature': best_features,\n",
    "    'Importance': pipeline_clf_cluster['model'].feature_importances_})\n",
    "    .sort_values(by='Importance', ascending=False)\n",
    ")\n",
    "\n",
    "best_features = df_feature_importance['Feature'].to_list()\n",
    "\n",
    "# Most important features statement and plot\n",
    "print(\n",
    "    f\"* These are the {len(best_features)} most important features \"\n",
    "    f\"in descending order:\\n{best_features}\\n\"\n",
    ")\n",
    "\n",
    "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the initial model, cluster formation was influenced by a more balanced mix of factors: default on file (50%), home ownership (30%), and income (20%).\n",
    "In the reduced model, home ownership and default on file now dominate equally with around 50% each, while income contributes only about 5%.\n",
    "\n",
    "This indicates that the simplified model primarily separates borrowers based on whether they have a history of default and their home ownership status. It effectively captures the main drivers of segmentation but loses some of the nuanced influence of income.\n",
    "\n",
    "While the reduced model is more efficient and suitable for real-time applications, it provides a narrower perspective on borrower behavior.\n",
    "\n",
    "Therefore, in a real-world scenario, stakeholders should evaluate whether this trade-off between simplicity and interpretability is acceptable. If the reduced model still meets business objectives, it can be preferred for operational use; otherwise, retaining the full model may be more valuable for deeper insights and decision-making.\n",
    "\n",
    "However, for the purpose of this project’s learning objective, we accept the less balanced mix of influencing factors, as it demonstrates the trade-offs involved when simplifying models for practical use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9q1TJSqdI6xK"
   },
   "source": [
    "## Cluster Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8EMIqE5I6xP"
   },
   "source": [
    "Create a DataFrame that contains the best features and Clusters Predictions: we want to analyse the patterns for each cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TEg92vdnI6xP"
   },
   "outputs": [],
   "source": [
    "df_cluster_profile = df_clf.copy()\n",
    "df_cluster_profile = (df_cluster_profile\n",
    "                      .filter(items=best_features + ['Clusters'], axis=1))\n",
    "df_cluster_profile.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "789CeA0WI6xQ"
   },
   "source": [
    "We want also to analyse Default levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nx335aqtI6xR"
   },
   "outputs": [],
   "source": [
    "df_default = pd.read_csv(\"outputs/datasets/collection/LoanDefaultData.csv\")\n",
    "df_default = df_default.drop_duplicates().filter([target_var])\n",
    "print(df_default.shape)\n",
    "df_default[target_var] = df_default[target_var].astype('object')\n",
    "df_default.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-7jYzhtI6xR"
   },
   "source": [
    "### Cluster profile on most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "urBmw5HJI6xS"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "clusters_profile = DescriptionAllClusters(\n",
    "    df= pd.concat([df_cluster_profile,df_default], axis=1), decimal_points=0)\n",
    "clusters_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster Description using the most important features:\n",
    "\n",
    "* Cluster 0: Borrowers with a history of previous defaults: mostly renters, with moderate incomes and the highest default rate. This group represents a higher-risk segment.\n",
    "* Cluster 1: Borrowers who mostly rent, have lower to mid-range incomes, and moderate default rates. They generally have no prior default record but represent a middle-risk segment.\n",
    "* Cluster 2: Borrowers who primarily have mortgages, earn higher incomes, and rarely default. This is the most financially stable and lowest-risk group.\n",
    "\n",
    ">Note: Compared to the earlier analysis, the labels for cluster 1 and 2 appear reversed, but the underlying groups and interpretations remain consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDJRuBBgI6xS"
   },
   "source": [
    "### Clusters distribution across Default levels & Relative Percentage of Default in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjMnAqYKI6xS"
   },
   "outputs": [],
   "source": [
    "df_cluster_vs_Default=  df_default.copy()\n",
    "df_cluster_vs_Default['Clusters'] = X['Clusters']\n",
    "cluster_distribution_per_variable(df=df_cluster_vs_Default, target=target_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only minor\n",
    "differences in the cluster profile. However, that is not significant and overall we can achieve the\n",
    "same interpretations from the previous pipeline. In relation to the cluster distribution\n",
    "across Default levels, there is no change between this and the previous pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xhPLbC4dwXL"
   },
   "source": [
    "## Which pipeline should I deploy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8qASh5k1jph"
   },
   "source": [
    "Let's recap the criteria we consider to evaluate the **trade-off**\n",
    "1. Apply the elbow method and silhouette analysis to see if the same number of clusters is suggested.\n",
    "    * The elbow and silhouette analysis both suggested k=3\n",
    "2. Fit the new cluster pipeline and compare its predictions to the previous pipeline to check for consistency.\n",
    "    * The confusion matrix showed the clusters were largely equivalent after relabeling\n",
    "3. Fit a classifier to explain the clusters and compare Train/Test performance to the original pipeline.\n",
    "4. Verify if the classifier identifies the same most important features as before.\n",
    "    * The classifier performance and interpretability were similar\n",
    "5. Compare the cluster profiles from both pipelines to check if they are equivalent.\n",
    "    * The cluster profiles were consistent (though with reversed labels and less dominance of income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4HsuSuqd0g_"
   },
   "outputs": [],
   "source": [
    "pipeline_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We considered 5 criteria to evaluate the trade-off when fitting the new pipeline using only the most important features.\n",
    "In this case, all five criteria were satisfactorily met. The elbow method and silhouette analysis both suggested k=3 clusters, the same as the previous pipeline. After relabeling, the cluster predictions were largely equivalent, and the classifier performance on the Train and Test sets remained consistent.\n",
    "\n",
    "Although the feature importance became more concentrated in home ownership and default on file compared to the previous model, the overall cluster profiles and their interpretations remained aligned with earlier findings (with reversed label assignments).\n",
    "\n",
    "Therefore, we can confidently select the reduced-feature pipeline, as it achieves comparable clustering quality while simplifying the model and reducing the number of variables needed for real-time applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zxpjktKd1n6"
   },
   "source": [
    "# Push files to Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5i9X1oOORAQc"
   },
   "source": [
    "\n",
    "We will generate the following files\n",
    "\n",
    "* Cluster Pipeline\n",
    "* Train Set\n",
    "* Feature importance plot\n",
    "* Clusters Description\n",
    "* Cluster Silhouette\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ySBIrV1Q4cY"
   },
   "outputs": [],
   "source": [
    "version = 'v2'\n",
    "file_path = f'outputs/ml_pipeline/cluster_analysis/{version}'\n",
    "\n",
    "try:\n",
    "    os.makedirs(name=file_path)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6y9-0fisd5cl"
   },
   "source": [
    "## Cluster pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xfv9k5xMd7fv"
   },
   "outputs": [],
   "source": [
    "pipeline_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IsphnIR84hJ4"
   },
   "outputs": [],
   "source": [
    "joblib.dump(value=pipeline_cluster, \n",
    "            filename=f\"{file_path}/cluster_pipeline.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ORnkwG6d74O"
   },
   "source": [
    "## Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QqcwHaVwd9Ff"
   },
   "outputs": [],
   "source": [
    "print(df_reduced.shape)\n",
    "df_reduced.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M26MiJ9Y485Q"
   },
   "outputs": [],
   "source": [
    "df_reduced.to_csv(f\"{file_path}/TrainSet.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eX_lcQVXaV0p"
   },
   "source": [
    "## Most important features plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9datNfsLCVV"
   },
   "source": [
    "These are the features that define a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYeoH7fjaV8J"
   },
   "outputs": [],
   "source": [
    "df_feature_importance.plot(kind='bar',x='Feature',y='Importance', figsize=(8,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sr0cVVQsaZqk"
   },
   "outputs": [],
   "source": [
    "df_feature_importance.plot(kind='bar',x='Feature',y='Importance', figsize=(8,4))\n",
    "plt.savefig(f\"{file_path}/features_define_cluster.png\", \n",
    "            bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GX3Z5ivNd9mw"
   },
   "source": [
    "## Cluster Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tw-mEnI8d_Bv"
   },
   "outputs": [],
   "source": [
    "clusters_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7G5CsAl738p7"
   },
   "outputs": [],
   "source": [
    "clusters_profile.to_csv(f\"{file_path}/clusters_profile.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RObeac1HQq5a"
   },
   "source": [
    "## Cluster silhouette plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = SilhouetteVisualizer(Pipeline(pipeline_cluster.steps[-1:])[0] , \n",
    "                                  colors='yellowbrick')\n",
    "visualizer.fit(df_analysis)\n",
    "visualizer.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(7,5))\n",
    "fig = SilhouetteVisualizer(Pipeline(pipeline_cluster.steps[-1:])[0] , \n",
    "                           colors='yellowbrick', ax=axes)\n",
    "fig.fit(df_analysis)\n",
    "\n",
    "plt.savefig(f\"{file_path}/clusters_silhouette.png\", bbox_inches='tight',dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "We applied a clustering pipeline to segment loan applicants based on key features. After preprocessing, dimensionality reduction with PCA, and KMeans clustering, we identified three main clusters:\n",
    "* Cluster 0: Borrowers with a history of previous defaults: mostly renters, with moderate incomes and the highest default rate. This group represents a higher-risk segment.\n",
    "* Cluster 1: Borrowers who mostly rent, have lower to mid-range incomes, and moderate default rates. They generally have no prior default record but represent a middle-risk segment.\n",
    "* Cluster 2: Borrowers who primarily have mortgages, earn higher incomes, and rarely default. This is the most financially stable and lowest-risk group.\n",
    "\n",
    "A classifier confirmed that the most important features defining the clusters are person_home_ownership and cb_person_default_on_file, with person_income playing a much smaller role. The final cluster model using only these features is robust, interpretable, and will be applied in the dashboard alongside our predictive default model. This allows us to segment borrowers and provide actionable insights in real time."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Modeling and Evaluation - Cluster Sklearn.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
