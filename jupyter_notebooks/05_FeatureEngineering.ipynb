{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Feature Engineering Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "*  Evaluate which transformations are beneficial for our dataset\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* inputs/datasets/cleaned/TrainSet.csv\n",
        "* inputs/datasets/cleaned/TestSet.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Generate a list of engineering approaches for each variable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ignore FutureWarnings\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder, where the notebook is stored, to its parent folder\n",
        "* First we access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "* Then we want to make the parent of the current directory the new current directory\n",
        "    * os.path.dirname() gets the parent directory\n",
        "    * os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "current_dir = os.getcwd()\n",
        "print(f\"You set a new current directory: {current_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Cleaned Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "file_path = \"outputs/datasets/cleaned\"\n",
        "\n",
        "TrainSet = pd.read_csv(f\"{file_path}/TrainSet.csv\")\n",
        "TrainSet.head(3)\n",
        "TrainSet.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TestSet = pd.read_csv(f\"{file_path}/TestSet.csv\")\n",
        "TestSet.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To identify potential transformations, we first revisit the Profile Report generated earlier.\n",
        "This allows us to assess:\n",
        "\n",
        "* The distributions of numerical variables (to detect skewness or outliers)\n",
        "* The cardinality and balance of categorical variables\n",
        "* Possible data scaling needs due to large differences in magnitude\n",
        "\n",
        "Based on these insights, we will determine which variables may benefit from transformations such as scaling, normalization or encoding before modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "    \n",
        "# Convert object columns to categorical so that it can be displayed properly in the report\n",
        "TrainSet_cat = TrainSet.copy()\n",
        "for col in TrainSet_cat.select_dtypes(include='object').columns:\n",
        "    TrainSet_cat[col] = TrainSet_cat[col].astype('category')\n",
        "    \n",
        "pandas_report = ProfileReport(df=TrainSet_cat, minimal=True)\n",
        "pandas_report.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ProfileReport suggests different transformations depending on the variable type and distribution.\n",
        "\n",
        "Categorical variables should be handled differently based on whether they are nominal or ordinal. The numerical variables are mostly uniformally distributed, so numerical transformations may help improve model performance. Additionally, numerical variables should be scaled to ensure comparable ranges, which benefits many machine learning algorithms. There are no outliers that have to be treated in this dataset. Finally, correlated features should be identified for possible removal to reduce redundancy.\n",
        "\n",
        "Transformation Steps:\n",
        "1. Categorical variables\n",
        "    * Nominal Variables: OneHotEncoder\n",
        "        * `EmploymentType`, `MaritalStatus`, `LoanPurpose`, `HasMortgage`, `HasDependents`, `HasCoSigner`\n",
        "    * Ordinal Variables: OrdinalEncoder\n",
        "        * `Education`\n",
        "2. Numerical Variables\n",
        "    * `NumCreditLines` and `LoanTerm`: No transformation as they have to be treated as ordinal categorical variables\n",
        "    * All other numerical variables: Numerical transformation, since they do not have a normal distribution \n",
        "3. All Variables: Smart correlated selection, so any correlated features will be removed\n",
        "4. Scaling variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import scipy.stats as stats\n",
        "# for vs code\n",
        "%matplotlib inline \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from feature_engine import transformation as vt\n",
        "from feature_engine.encoding import OneHotEncoder\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "sns.set(style=\"whitegrid\")\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Categorical Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Nominal Variables: OneHotEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 1: Select variables and create a separate DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "variables_engineering= [\"EmploymentType\", \"MaritalStatus\", \"LoanPurpose\", \"HasMortgage\", \"HasDependents\", \"HasCoSigner\"]\n",
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "df_engineering.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 2: Create engineered variables by applying the encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder = OneHotEncoder(variables=variables_engineering)\n",
        "df_feat_eng = encoder.fit_transform(df_engineering)\n",
        "df_feat_eng.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 3: Assess transformation by comparing engineered variables distribution to original ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for var in variables_engineering:\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import pandas as pd\n",
        "\n",
        "    # Count tables\n",
        "    raw_counts = df_engineering[var].value_counts().reset_index()\n",
        "    raw_counts.columns = [var, \"Count\"]\n",
        "\n",
        "    encoded_counts = (\n",
        "        df_feat_eng.filter(like=var, axis=1)\n",
        "        .sum()\n",
        "        .reset_index()\n",
        "    )\n",
        "    encoded_counts.columns = [\"Encoded Column\", \"Count\"]\n",
        "\n",
        "    # Create figure with 2 plots side by side\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
        "    fig.suptitle(f\"Distribution Comparison for '{var}' (Raw vs Encoded)\", fontsize=14, y=1.05)\n",
        "\n",
        "    # Raw variable countplot\n",
        "    sns.countplot(data=df_engineering, x=var, color=\"#432371\", ax=axes[0])\n",
        "    axes[0].set_title(\"Raw Variable (Countplot)\")\n",
        "    axes[0].set_xlabel(\"\")\n",
        "    axes[0].tick_params(axis=\"x\", rotation=90)\n",
        "\n",
        "    # Encoded variable bar plot\n",
        "    df_feat_eng.filter(like=var, axis=1).sum().plot(\n",
        "        kind=\"bar\", color=\"#432371\", width=0.8, ax=axes[1]\n",
        "    )\n",
        "    axes[1].set_title(\"Encoded Variables (One-Hot Columns)\")\n",
        "    axes[1].tick_params(axis=\"x\", rotation=90)\n",
        "    axes[1].set_xlabel(\"\")\n",
        "    axes[1].set_ylabel(\"Count\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- Show tables below the plots ---\n",
        "    print(\"Count Table (Raw Variable):\")\n",
        "    display(raw_counts)\n",
        "\n",
        "    print(\"\\nCount Table (Encoded Variables):\")\n",
        "    display(encoded_counts)\n",
        "    \n",
        "    print(\"---\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* For nominal categorical variables, the OneHotEncoder successfully transformed each category into separate binary columns. Therefore, this transformation will be applied in the pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 4 - Apply the selected transformation to the Train and Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder = OneHotEncoder(variables=variables_engineering)\n",
        "TrainSet = encoder.fit_transform(TrainSet)\n",
        "TestSet = encoder.fit_transform(TestSet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainSet.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TestSet.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ordinal Variables: OrdinalEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 1: Select variables and create a separate DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "variables_engineering= [\"Education\", \"Default\"]\n",
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "df_engineering.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 2: Create engineered variables by applying the encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# encoder = OrdinalEncoder(encoding_method='arbitrary', variables=variables_engineering)\n",
        "# df_feat_eng = encoder.fit_transform(df_engineering)\n",
        "# df_feat_eng.head(3)\n",
        "encoder = OrdinalEncoder(encoding_method='ordered', variables=\"Education\")\n",
        "df_feat_eng = encoder.fit_transform(pd.DataFrame(df_engineering[\"Education\"]), pd.DataFrame(df_engineering[\"Default\"]))\n",
        "df_feat_eng.head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 3: Assess transformation by comparing engineered variables distribution to original ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#for var in variables_engineering:\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "var = variables_engineering[0]\n",
        "# Count tables\n",
        "raw_counts = df_engineering[var].value_counts().reset_index()\n",
        "raw_counts.columns = [var, \"Count\"]\n",
        "\n",
        "encoded_counts = df_feat_eng[var].value_counts().reset_index()\n",
        "encoded_counts.columns = [var, \"Count\"]\n",
        "\n",
        "# Create figure with 2 plots side by side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
        "fig.suptitle(f\"Distribution Comparison for '{var}' (Raw vs Encoded)\", fontsize=14, y=1.05)\n",
        "\n",
        "# Raw variable countplot\n",
        "sns.countplot(data=df_engineering, x=var, color=\"#432371\", ax=axes[0])\n",
        "axes[0].set_title(\"Raw Variable (Countplot)\")\n",
        "axes[0].set_xlabel(\"\")\n",
        "axes[0].tick_params(axis=\"x\", rotation=90)\n",
        "\n",
        "# Encoded variable bar plot\n",
        "sns.countplot(data=df_feat_eng, x=var, color=\"#432371\", ax=axes[1])\n",
        "axes[0].set_title(\"Encoded Variable (Countplot)\")\n",
        "axes[0].set_xlabel(\"\")\n",
        "axes[0].tick_params(axis=\"x\", rotation=90)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Show tables below the plots\n",
        "print(\"Count Table (Raw Variable):\")\n",
        "display(raw_counts)\n",
        "\n",
        "print(\"\\nCount Table (Encoded Variables):\")\n",
        "display(encoded_counts)\n",
        "print(\"Class Mapping:\")\n",
        "print(encoder.encoder_dict_)\n",
        "\n",
        "print(\"---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The OrdinalEncoder successfully transformed the ordered categorical variables into numeric form while preserving their order. Therefore, this transformation will be applied in the pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 4 - Apply the selected transformation to the Train and Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder = OrdinalEncoder(encoding_method='ordered', variables=\"Education\")\n",
        "TrainSet[\"Education\"] = encoder.fit_transform(pd.DataFrame(TrainSet[\"Education\"]), pd.DataFrame(TrainSet[\"Default\"]))\n",
        "TestSet[\"Education\"] = encoder.transform(pd.DataFrame(TestSet[\"Education\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TestSet.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainSet.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Numerical Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Custom function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It gets a DataFrame as input and applies a defined set of numerical\n",
        "feature engineering transformers. This will help you to decide which transformers to apply to your data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO FUNKTION VEREINFACHEN??\n",
        "\n",
        "import scipy.stats as stats\n",
        "# for vs code\n",
        "%matplotlib inline \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from feature_engine import transformation as vt\n",
        "sns.set(style=\"whitegrid\")\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def FeatureEngineeringAnalysis(df, analysis_type=None):\n",
        "    \"\"\"\n",
        "    - used for quick feature engineering on numerical and categorical variables\n",
        "    to decide which transformation can better transform the distribution shape\n",
        "    - Once transformed, use a reporting tool, like ydata-profiling, to evaluate distributions\n",
        "    \"\"\"\n",
        "    check_missing_values(df)\n",
        "    allowed_types = ['numerical']\n",
        "    check_user_entry_on_analysis_type(analysis_type, allowed_types)\n",
        "    list_column_transformers = define_list_column_transformers(analysis_type)\n",
        "\n",
        "    # Loop in each variable and engineer the data according to the analysis type\n",
        "    df_feat_eng = pd.DataFrame([])\n",
        "    for column in df.columns:\n",
        "        # create additional columns (column_method) to apply the methods\n",
        "        df_feat_eng = pd.concat([df_feat_eng, df[column]], axis=1)\n",
        "        for method in list_column_transformers:\n",
        "            df_feat_eng[f\"{column}_{method}\"] = df[column]\n",
        "\n",
        "        # Apply transformers in respective column_transformers\n",
        "        df_feat_eng, list_applied_transformers = apply_transformers(\n",
        "            analysis_type, df_feat_eng, column)\n",
        "\n",
        "        # For each variable, assess how the transformations perform\n",
        "        transformer_evaluation(\n",
        "            column, list_applied_transformers, analysis_type, df_feat_eng)\n",
        "\n",
        "    return df_feat_eng\n",
        "\n",
        "\n",
        "def check_user_entry_on_analysis_type(analysis_type, allowed_types):\n",
        "    \"\"\" Check analysis type \"\"\"\n",
        "    if analysis_type is None:\n",
        "        raise SystemExit(\n",
        "            f\"You should pass analysis_type parameter as one of the following options: {allowed_types}\")\n",
        "    if analysis_type not in allowed_types:\n",
        "        raise SystemExit(\n",
        "            f\"analysis_type argument should be one of these options: {allowed_types}\")\n",
        "\n",
        "\n",
        "def check_missing_values(df):\n",
        "    if df.isna().sum().sum() != 0:\n",
        "        raise SystemExit(\n",
        "            f\"There is a missing value in your dataset. Please handle that before getting into feature engineering.\")\n",
        "\n",
        "\n",
        "def define_list_column_transformers(analysis_type):\n",
        "    \"\"\" Set suffix columns according to analysis_type\"\"\"\n",
        "    if analysis_type == 'numerical':\n",
        "        list_column_transformers = [\n",
        "            \"log_e\", \"log_10\", \"reciprocal\", \"power\", \"box_cox\", \"yeo_johnson\"]\n",
        "\n",
        "    return list_column_transformers\n",
        "\n",
        "\n",
        "def apply_transformers(analysis_type, df_feat_eng, column):\n",
        "\n",
        "    df_feat_eng, list_applied_transformers = FeatEngineering_Numerical(\n",
        "        df_feat_eng, column)\n",
        "\n",
        "    return df_feat_eng, list_applied_transformers\n",
        "\n",
        "\n",
        "def transformer_evaluation(column, list_applied_transformers, analysis_type, df_feat_eng):\n",
        "    # For each variable, assess how the transformations perform\n",
        "    print(f\"* Variable Analyzed: {column}\")\n",
        "    print(f\"* Applied transformation: {list_applied_transformers} \\n\")\n",
        "    for col in [column] + list_applied_transformers:\n",
        "\n",
        "        DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Numerical(df, variable):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    # sns.histplot(data=df, x=variable, kde=True, element=\"step\", ax=axes[0])\n",
        "    sns.histplot(data=df, x=variable, kde=True, ax=axes[0])\n",
        "    stats.probplot(df[variable], dist=\"norm\", plot=axes[1])\n",
        "\n",
        "    axes[0].set_title('Histogram')\n",
        "    axes[1].set_title('QQ Plot')\n",
        "    fig.suptitle(f\"{variable}\", fontsize=30, y=1.05)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def FeatEngineering_Numerical(df_feat_eng, column):\n",
        "    list_methods_worked = []\n",
        "\n",
        "    # LogTransformer base e\n",
        "    try:\n",
        "        lt = vt.LogTransformer(variables=[f\"{column}_log_e\"])\n",
        "        df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_log_e\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_log_e\"], axis=1, inplace=True)\n",
        "\n",
        "    # LogTransformer base 10\n",
        "    try:\n",
        "        lt = vt.LogTransformer(variables=[f\"{column}_log_10\"], base='10')\n",
        "        df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_log_10\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_log_10\"], axis=1, inplace=True)\n",
        "\n",
        "    # ReciprocalTransformer\n",
        "    try:\n",
        "        rt = vt.ReciprocalTransformer(variables=[f\"{column}_reciprocal\"])\n",
        "        df_feat_eng = rt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_reciprocal\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_reciprocal\"], axis=1, inplace=True)\n",
        "\n",
        "    # PowerTransformer\n",
        "    try:\n",
        "        pt = vt.PowerTransformer(variables=[f\"{column}_power\"])\n",
        "        df_feat_eng = pt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_power\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_power\"], axis=1, inplace=True)\n",
        "\n",
        "    # BoxCoxTransformer\n",
        "    try:\n",
        "        bct = vt.BoxCoxTransformer(variables=[f\"{column}_box_cox\"])\n",
        "        df_feat_eng = bct.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_box_cox\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_box_cox\"], axis=1, inplace=True)\n",
        "\n",
        "    # YeoJohnsonTransformer\n",
        "    try:\n",
        "        yjt = vt.YeoJohnsonTransformer(variables=[f\"{column}_yeo_johnson\"])\n",
        "        df_feat_eng = yjt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_yeo_johnson\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_yeo_johnson\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Numerical Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 1: Select variables and create a separate DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "variables_engineering= TrainSet.select_dtypes(include=['int64', 'float64']).columns.drop([\"NumCreditLines\", \"LoanTerm\", \"Default\"]).tolist()\n",
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "df_engineering.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 2: Create engineered variables by applying the encoder and assess transformation by comparing engineered variables distributions to original ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = FeatureEngineeringAnalysis(df=df_engineering, analysis_type='numerical')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Several numerical transformers were tested, including Log, Log base 10, Reciprocal, Power, Box-Cox and Yeo-Johnson. None of them helped to achieve a bell-shaped distribution or values aligned along the diagonal in the QQ plot. Therefore, no numerical transformation will be applied"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SmartCorrelatedSelection Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we're looking for groups of features that correlate amongst themselves,\n",
        "we want to remove any surplus correlated features since theyâ€™ll add the same information to the model.\n",
        "The transformer takes care of finding the groups and drops the features based on the method,\n",
        "threshold and selection method that we decided. This means for every group of correlated features,\n",
        "the transformer will remove all but one feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 1: Create a separate DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = TrainSet.copy()\n",
        "df_engineering.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confirm that all data types are numerical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 2: Create engineered variables applying the transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "corr_sel = SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.4, selection_method=\"variance\")\n",
        "\n",
        "corr_sel.fit_transform(df_engineering)\n",
        "corr_sel.correlated_feature_sets_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corr_sel.features_to_drop_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* After applying the `SmartCorrelatedSelection`, only the binary categorical variables showed high correlation, which is expected given their one-hot encoding. No other features were flagged for removal, which aligns with our earlier observations in the exploratory data analysis that overall correlation rates are very low. This confirms that multicollinearity is not a major concern for this dataset\n",
        "* Since the high correlation between dummy variables created by one-hot encoding can be resolved by setting ``drop_last=True`` in the `OneHotEncoder`, we will apply this parameter and omit the `SmartCorrelatedSelection` step. This simplifies the pipeline without affecting model correctness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scaling\n",
        "\n",
        "We apply scaling to the dataset so that all numerical variables are on a comparable range. This is important because many machine learning algorithms are sensitive to the magnitude of features. Without scaling, variables with larger ranges could dominate the learning process, leading to biased or suboptimal models.\n",
        "\n",
        "Note that after scaling, the especially categorical values are standardized and lose their original interpretability, but this ensures that all numerical inputs contribute comparably to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "df_engineering = TrainSet.copy()\n",
        "scaler = StandardScaler()\n",
        "scaled_array  = scaler.fit_transform(df_engineering)\n",
        "df_engineering = pd.DataFrame(scaled_array, columns=df_engineering.columns)\n",
        "df_engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering.describe().round(2).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Conclusions and Next Steps\n",
        "\n",
        "Transformation Steps we will apply:\n",
        "1. Categorical variables\n",
        "    * Nominal Variables: OneHotEncoder setting ``drop_last=True`` \n",
        "        * `EmploymentType`, `MaritalStatus`, `LoanPurpose`, `HasMortgage`, `HasDependents`, `HasCoSigner`\n",
        "    * Ordinal Variables: OrdinalEncoder\n",
        "        * `Education`\n",
        "2. Scaling variables\n",
        "\n",
        "Numerical variables will not be transformed further.\n",
        "\n",
        "Next Steps:\n",
        "* Model loan defaults and evaluate model performance"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
